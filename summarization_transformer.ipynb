{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:22:36.330143Z",
     "start_time": "2025-02-20T15:22:34.049087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# Check if GPU is available\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# import os\n"
   ],
   "id": "b25e86ebe92c96ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "GPU count: 1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load dataset",
   "id": "a446025fac5ce2dc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-20T15:22:40.369543Z",
     "start_time": "2025-02-20T15:22:36.330143Z"
    }
   },
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"argilla/cnn-dailymail-summaries\", split='train')\n",
    "print(ds)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['article', 'highlights', 'id', 'summary', 'distilabel_metadata', 'model_name'],\n",
      "    num_rows: 287113\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare data",
   "id": "87b02503926cda8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:22:41.571800Z",
     "start_time": "2025-02-20T15:22:40.531512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "articles = []\n",
    "summaries = []\n",
    "i = 0\n",
    "for data in ds:\n",
    "    articles.append(data['article'])\n",
    "    summaries.append(data['highlights'])\n",
    "    i += 1\n",
    "    if i == 10000:\n",
    "        break"
   ],
   "id": "a18148b6dc1234f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters.', 'The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they\\'re ready to appear in court. Most often, they face drug charges or charges of assaulting an officer --charges that Judge Steven Leifman says are usually \"avoidable felonies.\" He says the arrests often result from confrontations with police. Mentally ill people often won\\'t do what they\\'re told when police arrive on the scene -- confrontation seems to exacerbate their illness and they become more paranoid, delusional, and less likely to follow directions, according to Leifman. So, they end up on the ninth floor severely mentally disturbed, but not getting any real help because they\\'re in jail. We toured the jail with Leifman. He is well known in Miami as an advocate for justice and the mentally ill. Even though we were not exactly welcomed with open arms by the guards, we were given permission to shoot videotape and tour the floor.  Go inside the \\'forgotten floor\\' » . At first, it\\'s hard to determine where the people are. The prisoners are wearing sleeveless robes. Imagine cutting holes for arms and feet in a heavy wool sleeping bag -- that\\'s kind of what they look like. They\\'re designed to keep the mentally ill patients from injuring themselves. That\\'s also why they have no shoes, laces or mattresses. Leifman says about one-third of all people in Miami-Dade county jails are mentally ill. So, he says, the sheer volume is overwhelming the system, and the result is what we see on the ninth floor. Of course, it is a jail, so it\\'s not supposed to be warm and comforting, but the lights glare, the cells are tiny and it\\'s loud. We see two, sometimes three men -- sometimes in the robes, sometimes naked, lying or sitting in their cells. \"I am the son of the president. You need to get me out of here!\" one man shouts at me. He is absolutely serious, convinced that help is on the way -- if only he could reach the White House. Leifman tells me that these prisoner-patients will often circulate through the system, occasionally stabilizing in a mental hospital, only to return to jail to face their charges. It\\'s brutally unjust, in his mind, and he has become a strong advocate for changing things in Miami. Over a meal later, we talk about how things got this way for mental patients. Leifman says 200 years ago people were considered \"lunatics\" and they were locked up in jails even if they had no charges against them. They were just considered unfit to be in society. Over the years, he says, there was some public outcry, and the mentally ill were moved out of jails and into hospitals. But Leifman says many of these mental hospitals were so horrible they were shut down. Where did the patients go? Nowhere. The streets. They became, in many cases, the homeless, he says. They never got treatment. Leifman says in 1955 there were more than half a million people in state mental hospitals, and today that number has been reduced 90 percent, and 40,000 to 50,000 people are in mental hospitals. The judge says he\\'s working to change this. Starting in 2008, many inmates who would otherwise have been brought to the \"forgotten floor\"  will instead be sent to a new mental health facility -- the first step on a journey toward long-term treatment, not just punishment. Leifman says it\\'s not the complete answer, but it\\'s a start. Leifman says the best part is that it\\'s a win-win solution. The patients win, the families are relieved, and the state saves money by simply not cycling these prisoners through again and again. And, for Leifman, justice is served.', 'Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down,\" survivor Gary Babineau told CNN. \"I probably had a 30-, 35-foot free fall. And there\\'s cars in the water, there\\'s cars on fire. The whole bridge is down.\" He said his back was injured but he determined he could move around. \"I realized there was a school bus right next to me, and me and a couple of other guys went over and started lifting the kids off the bridge. They were yelling, screaming, bleeding. I think there were some broken bones.\"  Watch a driver describe his narrow escape » . At home when he heard about the disaster, Dr. John Hink, an emergency room physician, jumped into his car and rushed to the scene in 15 minutes. He arrived at the south side of the bridge, stood on the riverbank and saw dozens of people lying dazed on an expansive deck. They were in the middle of the Mississippi River, which was churning fast, and he had no way of getting to them. He went to the north side, where there was easier access to people. Ambulances were also having a hard time driving down to the river to get closer to the scene. Working feverishly, volunteers, EMTs and other officials managed to get 55 people into ambulances in less than two hours. Occasionally, a pickup truck with a medic inside would drive to get an injured person and bring him back up even ground, Hink told CNN. The rescue effort was controlled and organized, he said; the opposite of the lightning-quick collapse. \"I could see the whole bridge as it was going down, as it was falling,\" Babineau said. \"It just gave a rumble real quick, and it all just gave way, and it just fell completely, all the way to the ground. And there was dust everywhere and it was just like everyone has been saying: It was just like out of the movies.\" Babineau said the rear of his pickup truck was dangling over the edge of a broken-off section of the bridge. He said several vehicles slid past him into the water. \"I stayed in my car for one or two seconds. I saw a couple cars fall,\" he said. \"So I stayed in my car until the cars quit falling for a second, then I got out real quick, ran in front of my truck -- because behind my truck was just a hole -- and I helped a woman off of the bridge with me. \"I just wanted off the bridge, and then I ran over to the school bus. I started grabbing kids and handing them down. It was just complete chaos.\" He said most of the children were crying or screaming. He and other rescuers set them on the ground and told them to run to the river bank, but a few needed to be carried because of their injuries.  See rescuers clamber over rubble » . Babineau said he had no rescue training. \"I just knew what I had to do at the moment.\" Melissa Hughes, 32, of Minneapolis, told The Associated Press that she was driving home when the western edge of the bridge collapsed under her. \"You know that free-fall feeling? I felt that twice,\" Hughes said. A pickup landed on top of her car, but she was not hurt. \"I had no idea there was a vehicle on my car,\" she told AP. \"It\\'s really very surreal.\" Babineau told the Minneapolis Star-Tribune: \"On the way down, I thought I was dead. I literally thought I was dead. \"My truck was completely face down, pointed toward the ground, and my truck got ripped in half. It was folded in half, and I can\\'t believe I\\'m alive.\"  See and hear eyewitness accounts » . Bernie Toivonen told CNN\\'s \"American Morning\" that his vehicle was on a part of the bridge that ended up tilted at a 45-degree angle. \"I knew the deck was going down, there was no question about it, and I thought I was going to die,\" he said. After the bridge settled and his car remained upright, \"I just put in park, turned the key off and said, \\'Oh, I\\'m alive,\\' \" he said.', 'Doctors removed five small polyps from President Bush\\'s colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the National Naval Medical Center in Bethesda, Maryland, for routine microscopic examination, spokesman Scott Stanzel said. Results are expected in two to three days. All were small, less than a centimeter [half an inch] in diameter, he said. Bush is in good humor, Stanzel said, and will resume his activities at Camp David. During the procedure Vice President Dick Cheney assumed presidential power. Bush reclaimed presidential power at 9:21 a.m. after about two hours. Doctors used \"monitored anesthesia care,\" Stanzel said, so the president was asleep, but not as deeply unconscious as with a true general anesthetic. He spoke to first lady Laura Bush -- who is in Midland, Texas, celebrating her mother\\'s birthday -- before and after the procedure, Stanzel said. Afterward, the president played with his Scottish terriers, Barney and Miss Beazley, Stanzel said. He planned to have lunch at Camp David and have briefings with National Security Adviser Stephen Hadley and White House Chief of Staff Josh Bolten, and planned to take a bicycle ride Saturday afternoon. Cheney, meanwhile, spent the morning at his home on Maryland\\'s eastern shore, reading and playing with his dogs, Stanzel said. Nothing occurred that required him to take official action as president before Bush reclaimed presidential power. The procedure was supervised by Dr. Richard Tubb, Bush\\'s physician, and conducted by a multidisciplinary team from the National Naval Medical Center in Bethesda, Maryland, the White House said. Bush\\'s last colonoscopy was in June 2002, and no abnormalities were found, White House spokesman Tony Snow said. The president\\'s doctor had recommended a repeat procedure in about five years. A colonoscopy is the most sensitive test for colon cancer, rectal cancer and polyps, small clumps of cells that can become cancerous, according to the Mayo Clinic. Small polyps may be removed during the procedure. Snow said on Friday that Bush had polyps removed during colonoscopies before becoming president. Snow himself is undergoing chemotherapy for cancer that began in his colon and spread to his liver.  Watch Snow talk about Bush\\'s procedure and his own colon cancer » . \"The president wants to encourage everybody to use surveillance,\" Snow said. The American Cancer Society recommends that people without high risk factors or symptoms begin getting screened for signs of colorectal cancer at age 50.', 'The National Football League has indefinitely suspended Atlanta Falcons quarterback Michael Vick without pay, officials with the league said Friday. NFL star Michael Vick is set to appear in court Monday. A judge will have the final say on a plea deal. Earlier, Vick admitted to participating in a dogfighting ring as part of a plea agreement with federal prosecutors in Virginia. \"Your admitted conduct was not only illegal, but also cruel and reprehensible. Your team, the NFL, and NFL fans have all been hurt by your actions,\" NFL Commissioner Roger Goodell said in a letter to Vick. Goodell said he would review the status of the suspension after the legal proceedings are over. In papers filed Friday with a federal court in Virginia, Vick also admitted that he and two co-conspirators killed dogs that did not fight well. Falcons owner Arthur Blank said Vick\\'s admissions describe actions that are \"incomprehensible and unacceptable.\" The suspension makes \"a strong statement that conduct which tarnishes the good reputation of the NFL will not be tolerated,\" he said in a statement.  Watch what led to Vick\\'s suspension » . Goodell said the Falcons could \"assert any claims or remedies\" to recover $22 million of Vick\\'s signing bonus from the 10-year, $130 million contract he signed in 2004, according to The Associated Press. Vick said he would plead guilty to one count of \"Conspiracy to Travel in Interstate Commerce in Aid of Unlawful Activities and to Sponsor a Dog in an Animal Fighting Venture\" in a plea agreement filed at U.S. District Court in Richmond, Virginia. The charge is punishable by up to five years in prison, a $250,000 fine, \"full restitution, a special assessment and 3 years of supervised release,\" the plea deal said. Federal prosecutors agreed to ask for the low end of the sentencing guidelines. \"The defendant will plead guilty because the defendant is in fact guilty of the charged offense,\" the plea agreement said. In an additional summary of facts, signed by Vick and filed with the agreement, Vick admitted buying pit bulls and the property used for training and fighting the dogs, but the statement said he did not bet on the fights or receive any of the money won. \"Most of the \\'Bad Newz Kennels\\' operations and gambling monies were provided by Vick,\" the official summary of facts said. Gambling wins were generally split among co-conspirators Tony Taylor, Quanis Phillips and sometimes Purnell Peace, it continued. \"Vick did not gamble by placing side bets on any of the fights. Vick did not receive any of the proceeds from the purses that were won by \\'Bad Newz Kennels.\\' \" Vick also agreed that \"collective efforts\" by him and two others caused the deaths of at least six dogs. Around April, Vick, Peace and Phillips tested some dogs in fighting sessions at Vick\\'s property in Virginia, the statement said. \"Peace, Phillips and Vick agreed to the killing of approximately 6-8 dogs that did not perform well in \\'testing\\' sessions at 1915 Moonlight Road and all of those dogs were killed by various methods, including hanging and drowning. \"Vick agrees and stipulates that these dogs all died as a result of the collective efforts of Peace, Phillips and Vick,\" the summary said. Peace, 35, of Virginia Beach, Virginia; Phillips, 28, of Atlanta, Georgia; and Taylor, 34, of Hampton, Virginia, already have accepted agreements to plead guilty in exchange for reduced sentences. Vick, 27, is scheduled to appear Monday in court, where he is expected to plead guilty before a judge.  See a timeline of the case against Vick » . The judge in the case will have the final say over the plea agreement. The federal case against Vick focused on the interstate conspiracy, but Vick\\'s admission that he was involved in the killing of dogs could lead to local charges, according to CNN legal analyst Jeffrey Toobin. \"It sometimes happens -- not often -- that the state will follow a federal prosecution by charging its own crimes for exactly the same behavior,\" Toobin said Friday. \"The risk for Vick is, if he makes admissions in his federal guilty plea, the state of Virginia could say, \\'Hey, look, you admitted violating Virginia state law as well. We\\'re going to introduce that against you and charge you in our court.\\' \" In the plea deal, Vick agreed to cooperate with investigators and provide all information he may have on any criminal activity and to testify if necessary. Vick also agreed to turn over any documents he has and to submit to polygraph tests. Vick agreed to \"make restitution for the full amount of the costs associated\" with the dogs that are being held by the government. \"Such costs may include, but are not limited to, all costs associated with the care of the dogs involved in that case, including if necessary, the long-term care and/or the humane euthanasia of some or all of those animals.\" Prosecutors, with the support of animal rights activists, have asked for permission to euthanize the dogs. But the dogs could serve as important evidence in the cases against Vick and his admitted co-conspirators. Judge Henry E. Hudson issued an order Thursday telling the U.S. Marshals Service to \"arrest and seize the defendant property, and use discretion and whatever means appropriate to protect and maintain said defendant property.\" Both the judge\\'s order and Vick\\'s filing refer to \"approximately\" 53 pit bull dogs. After Vick\\'s indictment last month, Goodell ordered the quarterback not to report to the Falcons training camp, and the league is reviewing the case. Blank told the NFL Network on Monday he could not speculate on Vick\\'s future as a Falcon, at least not until he had seen \"a statement of facts\" in the case. ', 'Dressed in a Superman shirt, 5-year-old Youssif held his sister\\'s hand Friday, seemingly unaware that millions of people across the world have been touched by his story. Nearby, his parents talked about the new future and hope they have for their boy -- and the potential for recovery from his severe burns. Youssif holds his sister\\'s hand Friday. He\\'s wearing a facial mask often used to help burn victims. It\\'s the best birthday present the Iraqi family could ever have imagined for their boy: Youssif turns 6 next Friday. \"I was so happy I didn\\'t know what to do with myself,\" his mother, Zainab, told CNN, a broad smile across her face. \"I didn\\'t think the reaction would be this big.\" His father said he was on the roof of his house when CNN called him with the news about the outpouring of support for his son. \"We just want to thank everyone who has come forward,\" he said. \"We knew there was kindness out there.\" Like his wife, he couldn\\'t stop smiling. He talked about how he tried in vain to get help for his son in Baghdad, leaving \"no stone unturned\" on a mission to help his boy. There were many trips to the Ministry of Health. He says he even put in a request to Iraq\\'s parliament for help. The family eventually told CNN their story -- that Youssif was grabbed by masked men outside their home on January 15, doused in gasoline and set on fire. Simply by coming forward, his parents put themselves in incredible danger. No one has been arrested or held accountable in Youssif\\'s case.  Watch CNN\\'s Arwa Damon describe \\'truly phenomenal\\' outpouring » . Shortly after Youssif\\'s story aired Wednesday, the Children\\'s Burn Foundation -- a nonprofit organization based in Sherman Oaks, California, that provides support for burn victims locally, nationally and internationally -- agreed to pay for the transportation for Youssif and his family to come to the United States and to set up a fund for donations. You can make a donation at the foundation\\'s site by clicking here. There\\'s a drop-down menu under the \"general donation\" area that is marked \"Youssif\\'s fund.\" The foundation says it will cover all medical costs -- from surgeries for Youssif to housing costs to any social rehabilitation that might be needed for him. Surgeries will be performed by Dr. Peter Grossman, a plastic surgeon with the affiliated Grossman Burn Center who is donating his services for Youssif\\'s cause. Officials are still trying to get the appropriate visas for the family\\'s travels. \"We are prepared to have them come here, set them up in a housing situation, provide support for them and begin treatment,\" said Barbara Friedman, executive director of the Children\\'s Burn Foundation. \"We expect that the treatment will be from between six months to a year with many surgeries.\" She added, \"He will be getting the absolute best care that\\'s available.\" Youssif\\'s parents said they know it\\'s going to be a lengthy and difficult process and that adjusting to their stay in America may not be easy. But none of that matters -- getting help for their boy is first and foremost. \"I will do anything for Youssif,\" his father said, pulling his son closer to him. \"Our child is everything.\" His mother tried to coax Youssif to talk to us on this day. But he didn\\'t want to; his mother says he\\'s shy outside of their home. The biggest obstacle now is getting the visas to leave, and the serious security risks they face every day and hour they remain in Iraq. But this family -- which saw the very worst in humanity on that January day -- has new hope in the world. That is partly due to the tens of thousands of CNN.com users who were so moved by the story and wanted to act. CNN Iraqi staff central to bringing this story together were also overwhelmed with the generosity coming from people outside of their border. In a nation that largely feels abandoned by the rest of the world, it was a refreshing realization.', 'The women are too afraid and ashamed to show their faces or have their real names used. They have been driven to sell their bodies to put food on the table for their children -- for as little as $8 a day. Suha, 37, is a mother of three. She says her husband thinks she is cleaning houses when she leaves home. \"People shouldn\\'t criticize women, or talk badly about them,\" says 37-year-old Suha as she adjusts the light colored scarf she wears these days to avoid extremists who insist women cover themselves. \"They all say we have lost our way, but they never ask why we had to take this path.\" A mother of three, she wears light makeup, a gold pendant of Iraq around her neck, and an unexpected air of elegance about her. \"I don\\'t have money to take my kid to the doctor. I have to do anything that I can to preserve my child, because I am a mother,\" she says, explaining why she prostitutes herself. Anger and frustration rise in her voice as she speaks. \"No matter what else I may be, no matter how off the path I may be, I am a mother!\"  Watch a woman describe turning to prostitution to \"save my child\" » . Her clasped hands clench and unclench nervously. Suha\\'s husband thinks that she is cleaning houses when she goes away. So does Karima\\'s family. \"At the start I was cleaning homes, but I wasn\\'t making much. No matter how hard I worked it just wasn\\'t enough,\" she says. Karima, clad in all black, adds, \"My husband died of lung cancer nine months ago and left me with nothing.\" She has five children, ages 8 to 17. Her eldest son could work, but she\\'s too afraid for his life to let him go into the streets, preferring to sacrifice herself than risk her child. She was solicited the first time when she was cleaning an office. \"They took advantage of me,\" she says softly. \"At first I rejected it, but then I realized I have to do it.\" Both Suha and Karima have clients that call them a couple times a week. Other women resort to trips to the market to find potential clients. Or they flag down vehicles. Prostitution is a choice more and more Iraqi women are making just to survive. \"It\\'s increasing,\" Suha says. \"I found this \\'thing\\' through my friend, and I have another friend in the same predicament as mine. Because of the circumstance, she is forced to do such things.\" Violence, increased cost of living, and lack of any sort of government aid leave women like these with few other options, according to humanitarian workers. \"At this point there is a population of women who have to sell their bodies in order to keep their children alive,\" says Yanar Mohammed, head and founder of the Organization for Women\\'s Freedom in Iraq. \"It\\'s a taboo that no one is speaking about.\" She adds, \"There is a huge population of women who were the victims of war who had to sell their bodies, their souls and they lost it all. It crushes us to see them, but we have to work on it and that\\'s why we started our team of women activists.\" Her team pounds the streets of Baghdad looking for these victims often too humiliated to come forward. \"Most of the women that we find at hospitals [who] have tried to commit suicide\" have been involved in prostitution, said Basma Rahim, a member of Mohammed\\'s team. The team\\'s aim is to compile information on specific cases and present it to Iraq\\'s political parties -- to have them, as Mohammed puts it, \"come tell us what [they] are ... going to do about this.\" Rahim tells the heartbreaking story of one woman they found who lives in a room with three of her children: \"She has sex while her three children are in the room, but she makes them stand in separate corners.\" According to Rahim and Mohammed, most of the women they encounter say they are driven to prostitution by a desperate desire for survival in the dangerously violent and unforgiving circumstances in Iraq. \"They took this path but they are not pleased,\" Rahim says. Karima says when she sees her children with food on the table, she is able to convince herself that it\\'s worth it. \"Everything is for the children. They are the beauty in life and, without them, we cannot live.\" But she says, \"I would never allow my daughter to do this. I would rather marry her off at 13 than have her go through this.\" Karima\\'s last happy memory is of her late husband, when they were a family and able to shoulder the hardships of life in today\\'s Iraq together. Suha says as a young girl she dreamed of being a doctor, with her mom boasting about her potential in that career. Life couldn\\'t have taken her further from that dream. \"It\\'s not like we were born into this, nor was it ever in my blood,\" she says. What she does for her family to survive now eats away at her. \"I lay on my pillow and my brain is spinning, and it all comes back to me as if I am watching a movie.\"', 'A key rebel commander and fugitive from a U.S. drug trafficking indictment was killed over the weekend in an air attack on a guerrilla encampment, the Colombian military said Monday. Alleged cocaine trafficker and FARC rebel Tomas Medina Caracas in an Interpol photo. Tomas Medina Caracas, known popularly as \"El Negro Acacio,\" was a member of the high command of the Fuerzas Armadas Revolucionarias de Colombia and, according to Colombian and U.S. officials, helped manage the group\\'s extensive cocaine trafficking network. He had been in the cross-hairs of the U.S. Justice Department since 2002. He was charged with conspiracy to import cocaine into the United States and manufacturing and distributing cocaine within Colombia to fund the FARC\\'s 42-year insurgency against the government. U.S. officials alleged Medina Caracas managed the rebel group\\'s sales of cocaine to international drug traffickers, who in turn smuggled it into the United States. He was also indicted in the United States along with two other FARC commanders in November 2002 on charges of conspiring to kidnap two U.S. oil workers from neighboring Venezuela in 1997 and holding one of them for nine months until a $1 million ransom was paid. Officials said the army\\'s Rapid Response Force, backed by elements of the Colombian Air Force, tracked Medina Caracas down at a FARC camp in the jungle in the south of the country. \"After a bombardment, the troops occupied the camp, and they\\'ve found 14 dead rebels so far, along with rifles, pistols, communications equipment and ... four GPS systems,\" Defense Minister Juan Manuel Santos said at a news conference. \"The death of \\'El Negro Acacio\\' was confirmed by various sources, including members of FARC itself.\" Medina Caracas commanded FARC\\'s 16th Front in the southern departments of Vichada and Guainia. Established in 1964 as the military wing of the Colombian Communist Party, FARC is Colombia\\'s oldest, largest, most capable and best-equipped Marxist rebel group, according to the U.S. Department of State.', 'White House press secretary Tony Snow, who is undergoing treatment for cancer, will step down from his post September 14 and be replaced by deputy press secretary Dana Perino, the White House announced Friday. White House press secretary Tony Snow will step down from his post on September 14. President Bush told reporters Friday that he will \"sadly accept\" Snow\\'s resignation. Flanked by Snow and Perino in the White House press room, the president spoke warmly of his departing press secretary. \"It\\'s been a joy to watch him spar with you,\" Bush told reporters.  Watch the announcement about Snow leaving » . Bush said he was certain of two things in regard to Snow. \"He\\'ll battle cancer and win,\" Bush said, \"and he\\'ll be a solid contributor to society.\" Turning to Snow, the president then said: \"I love you, and I wish you all the best.\" Snow, speaking after Bush at the start of the daily White House news conference, said he was leaving to earn more money. He took a big pay cut, he said, when he left his previous jobs as anchor and political analyst for Fox News. According to The Washington Post, Snow makes $168,000 as the White House spokesman. His family took out a loan when he started the job, \"and that loan is now gone.\" \"This job has really been a dream for me, a blast. I\\'ve had an enormous amount of fun and satisfaction,\" Snow said. He said he would continue to speak out on issues, and would do \"some radio, some TV, but I don\\'t anticipate full-time anchor duties.\" Snow said he\\'s received great satisfaction from talking to people about his illness. Snow\\'s cancer was diagnosed for the first time in February 2005. His colon was removed, and after six months of treatment, doctors said the cancer was in remission. Perino announced March 27 that Snow\\'s cancer had recurred, and that doctors had removed a growth from his abdomen the day before. Sources told CNN two weeks ago that Snow was planning to leave his job, possibly as early as September. Bush tapped Snow to replace Scott McClellan in April 2006. Snow had been an anchor for \"Fox News Sunday\" and a political analyst for the Fox News Channel, which he joined in 1996. He also hosted \"The Tony Snow Show\" on Fox News Radio. On Thursday, Snow told CNN his health is improving, citing two medical tests this month that found the cancer has not spread. \"The tumors are stable -- they are not growing,\" Snow said of the results from an MRI and a CAT scan. \"And there are no new growths. The health is good.\" The press secretary, whose hair has turned gray during chemotherapy treatment, said his black hair is expected to grow back in about a month. \"I\\'m also putting on weight again,\" he said after returning from a 10-day vacation. \"I actually feel very good about\" the health situation. Snow said on Friday he was to see his oncologist, and they will decide on some minor forms of chemotherapy to start as maintenance treatment.', 'Police and FBI agents are investigating the discovery of an empty rocket launcher tube on the front lawn of a Jersey City, New Jersey, home, FBI spokesman Sean Quinn said. Niranjan Desai discovered the 20-year-old AT4 anti-tank rocket launcher tube, a one-time-use device, lying on her lawn Friday morning, police said. The launcher has been turned over to U.S. Army officials at the 754th Ordnance Company, an explosive ordnance disposal unit, at Fort Monmouth, New Jersey, Army officials said. The launcher \"is no longer operable and not considered to be a hazard to public safety,\" police said, adding there was no indication the launcher had been fired recently. Army officials said they could not determine if the launcher had been fired, but indicated they should know once they find out where it came from. The nearest military base, Fort Dix, is more than 70 miles from Jersey City. The Joint Terrorism Task Force division of the FBI and Jersey City police are investigating the origin of the rocket launcher and the circumstance that led to its appearance on residential property. \"Al Qaeda doesn\\'t leave a rocket launcher on the lawn of middle-aged ladies,\" said Paul Cruickshank of New York University Law School\\'s Center on Law and Security. A neighbor, Joe Quinn, said the object lying on Desai\\'s lawn looked military, was brown, had a handle and strap, and \"both ends were open, like you could shoot something with it.\" Quinn also said the device had a picture of a soldier on it and was 3 to 4 feet long. An Army official said the device is basically a shoulder-fired, direct-fire weapon used against ground targets -- a modern-day bazooka -- and it is not wire-guided. According to the Web site Globalsecurity.org, a loaded M136 AT4 anti-tank weapon has a 40-inch-long fiberglass-wrapped tube and weighs just 4 pounds. Its 84 millimeter shaped-charge missile can penetrate 14 inches of armor from a maximum of 985 feet. It is used once and discarded.']\n",
      "[\"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\", 'Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he\\'s fighting for change .', 'NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .', 'Five small polyps found during procedure; \"none worrisome,\" spokesman says .\\nPresident reclaims powers transferred to vice president .\\nBush undergoes routine colonoscopy at Camp David .', \"NEW: NFL chief, Atlanta Falcons owner critical of Michael Vick's conduct .\\nNFL suspends Falcons quarterback indefinitely without pay .\\nVick admits funding dogfighting operation but says he did not gamble .\\nVick due in federal court Monday; future in NFL remains uncertain .\", 'Parents beam with pride, can\\'t stop from smiling from outpouring of support .\\nMom: \"I was so happy I didn\\'t know what to do\"\\nBurn center in U.S. has offered to provide treatment for reconstructive surgeries .\\nDad says, \"Anything for Youssif\"', 'Aid workers: Violence, increased cost of living drive women to prostitution .\\nGroup is working to raise awareness of the problem with Iraq\\'s political leaders .\\nTwo Iraqi mothers tell CNN they turned to prostitution to help feed their children .\\n\"Everything is for the children,\" one woman says .', 'Tomas Medina Caracas was a fugitive from a U.S. drug trafficking indictment .\\n\"El Negro Acacio\" allegedly helped manage extensive cocaine network .\\nU.S. Justice Department indicted him in 2002 .\\nColombian military: He was killed in an attack on a guerrilla encampment .', 'President Bush says Tony Snow \"will battle cancer and win\"  Job of press secretary \"has been a dream for me,\" Snow says  Snow leaving on September 14, will be succeeded by Dana Perino .', 'Empty anti-tank weapon turns up in front of New Jersey home .\\nDevice handed over to Army ordnance disposal unit .\\nWeapon not capable of being reloaded, experts say .']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get Tokenizer - BartTokenizer with 50265 vocab size",
   "id": "b63c8a35c64f0a7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:22:43.889363Z",
     "start_time": "2025-02-20T15:22:41.577568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "SRC_MAX_SEQ = 1024\n",
    "TGT_MAX_SEQ = 128\n"
   ],
   "id": "bd29edde37c72d6c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clean data",
   "id": "758e78e09478c0da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:46.033840Z",
     "start_time": "2025-02-20T15:22:43.896112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"To set determined len for src and tgt --> also with pad tokens\"\"\"\n",
    "\n",
    "inputs = [tokenizer.encode(str(sentence)) for sentence in articles]\n",
    "outputs = [tokenizer.encode(str(sentence)) for sentence in summaries]\n",
    "\n",
    "\n",
    "def clean_data(sentences, max_seq_length):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(sentence) < max_seq_length:\n",
    "            sentences[i] = sentence + [tokenizer.pad_token_id] * (max_seq_length - len(sentence))\n",
    "        elif len(sentence) > max_seq_length:\n",
    "            sentences[i] = sentence[:max_seq_length - 1] + [tokenizer.eos_token_id]\n",
    "\n",
    "\n",
    "clean_data(inputs, SRC_MAX_SEQ)\n",
    "clean_data(outputs, TGT_MAX_SEQ)"
   ],
   "id": "13fddb44c3ac001c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:48.138432Z",
     "start_time": "2025-02-20T15:23:46.105234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "articles_inputs = torch.tensor(inputs)\n",
    "summaries_outputs = torch.tensor(outputs)\n",
    "assert len(articles_inputs) == len(summaries_outputs), \"Number of articles and summaries must be the same\"\n",
    "print('Max src seq len:', SRC_MAX_SEQ)\n",
    "print('Max tgt seq len:', TGT_MAX_SEQ)\n",
    "print('Number of examples:', len(inputs))\n",
    "print(\"Number of vocab size:\", VOCAB_SIZE)\n"
   ],
   "id": "b871c29a18bd72e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max src seq len: 1024\n",
      "Max tgt seq len: 128\n",
      "Number of examples: 10000\n",
      "Number of vocab size: 50265\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Custom Dataset and DataLoader with train and val data (80%, 20%)",
   "id": "d5f9a69697ca4da7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:48.148203Z",
     "start_time": "2025-02-20T15:23:48.138432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split, Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs: torch.Tensor, outputs: torch.Tensor):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "\n",
    "dataset = CustomDataset(articles_inputs, summaries_outputs)\n",
    "train_data, val_data = random_split(dataset, [0.8, 0.2])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(val_data)}\")"
   ],
   "id": "482d5e998d8ec790",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8000\n",
      "Number of validation samples: 2000\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:48.172320Z",
     "start_time": "2025-02-20T15:23:48.158544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in training set: {len(train_loader)}\")\n",
    "print(f\"Number of batches in validation set: {len(val_loader)}\")\n"
   ],
   "id": "cd381a10172d1af3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in training set: 500\n",
      "Number of batches in validation set: 125\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:48.197099Z",
     "start_time": "2025-02-20T15:23:48.180385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model  # Model's dimension\n",
    "        self.num_heads = num_heads  # Number of attention heads\n",
    "        self.d_k = d_model // num_heads  # Dimension of each head's key, query, and value\n",
    "\n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Output transformation\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ],
   "id": "ef607fe7d7712cd5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:48.221546Z",
     "start_time": "2025-02-20T15:23:48.205671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ],
   "id": "5785cc712765f655",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:48.239563Z",
     "start_time": "2025-02-20T15:23:48.229880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"NOTE: There is no evidence that positional encoding is better than simple learnable embeddings.\"\"\"\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ],
   "id": "fe4e61c154ff8b1a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:48.254581Z",
     "start_time": "2025-02-20T15:23:48.247553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ],
   "id": "67a0b8abf7ef1b80",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:48.271796Z",
     "start_time": "2025-02-20T15:23:48.263258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ],
   "id": "13f775b689e91e6e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:48.319600Z",
     "start_time": "2025-02-20T15:23:48.297529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n",
    "                 max_tgt_seq_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.src_positional_encoding = PositionalEncoding(d_model, max_src_seq_len)\n",
    "        self.tgt_positional_encoding = PositionalEncoding(d_model, max_tgt_seq_len)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(3)\n",
    "        src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        nopeak_mask = nopeak_mask.to(device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.src_positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.tgt_positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ],
   "id": "1f6b502c5537500e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:48.332807Z",
     "start_time": "2025-02-20T15:23:48.327858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_vocab_size = VOCAB_SIZE\n",
    "tgt_vocab_size = VOCAB_SIZE\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_src_seq_len = SRC_MAX_SEQ\n",
    "max_tgt_seq_len = TGT_MAX_SEQ\n",
    "dropout = 0.2"
   ],
   "id": "88b2eaebd84a3627",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T15:23:49.280181Z",
     "start_time": "2025-02-20T15:23:48.345775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n",
    "                          max_tgt_seq_len,\n",
    "                          dropout)\n",
    "transformer.to(device)"
   ],
   "id": "930121124b5e2e91",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(50265, 512)\n",
       "  (decoder_embedding): Embedding(50265, 512)\n",
       "  (src_positional_encoding): PositionalEncoding()\n",
       "  (tgt_positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=50265, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train model",
   "id": "a977685c1564feae"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-20T15:23:49.290029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "lr = 3e-4\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=lr, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "transformer.train()\n",
    "lr_history = []\n",
    "loss_history = []\n",
    "i = 0\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for src_data, tgt_data in progress_bar:\n",
    "        src_data = src_data.to(device)\n",
    "        tgt_data = tgt_data.to(device)\n",
    "\n",
    "        output = transformer(src_data, tgt_data[:, :-1])\n",
    "        loss = F.cross_entropy(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1),\n",
    "                               ignore_index=tokenizer.pad_token_id)\n",
    "        lr_history.append(optimizer.param_groups[0]['lr'])\n",
    "        loss_history.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)  # Prevent exploding gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        # i += 1\n",
    "        # if i == 100:\n",
    "        #     break\n",
    "\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {epoch_loss / len(train_loader):.4f}\")"
   ],
   "id": "4c27c273e453e232",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  36%|███▌      | 179/500 [09:39<17:29,  3.27s/it, Batch Loss=7.34]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Show charts with lr and loss",
   "id": "83ebbec4b3c21794"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Ensure lr_history and loss_history are lists of equal length\n",
    "assert len(lr_history) == len(loss_history), \"Length of lr_history and loss_history must be the same\"\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "# Plot Loss on primary y-axis\n",
    "ax1.set_title(\"Learning Rate vs. Loss\")\n",
    "ax1.set_xlabel(\"Training Step\")\n",
    "ax1.set_ylabel(\"Loss\", color='tab:red')\n",
    "ax1.plot(range(len(loss_history)), loss_history, color='tab:red', label='Loss')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Plot Learning Rate on secondary y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Learning Rate\", color='tab:blue')\n",
    "ax2.plot(range(len(lr_history)), lr_history, color='tab:blue', linestyle='--', label='Learning Rate')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Legends\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "d9ea000c71a19b90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate loss on validation data",
   "id": "c6f9c0e0e3f8a15d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "transformer.eval()\n",
    "\n",
    "total_val_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Use tqdm for progress bar\n",
    "    progress_bar = tqdm(val_loader, desc=\"Validating\")\n",
    "\n",
    "    for val_src_data, val_tgt_data in progress_bar:\n",
    "        # Move data to GPU\n",
    "        val_src_data, val_tgt_data = val_src_data.to(device), val_tgt_data.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "\n",
    "        # Calculate loss\n",
    "        val_loss = F.cross_entropy(\n",
    "            val_output.contiguous().view(-1, tgt_vocab_size),\n",
    "            val_tgt_data[:, 1:].contiguous().view(-1),\n",
    "            ignore_index=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        total_val_loss += val_loss.item()\n",
    "\n",
    "        # Update progress bar with current batch loss\n",
    "        progress_bar.set_postfix({\"Batch Loss\": val_loss.item()})\n",
    "\n",
    "# Calculate average validation loss\n",
    "avg_val_loss = total_val_loss / len(val_loader)\n",
    "print(f\"Average Validation Loss: {avg_val_loss:.4f}\")"
   ],
   "id": "66ea3fdd89fb08b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model inference",
   "id": "98b42d308d6a31e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_summary(sentence, tokenizer, model, max_length=TGT_MAX_SEQ):\n",
    "    \"\"\"\n",
    "    Translates a single Polish sentence into Ukrainian using greedy decoding.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Tokenize and convert to tensor\n",
    "    tokens = tokenizer.encode(sentence)\n",
    "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n",
    "    # print(tokens)\n",
    "\n",
    "    # Start with the input sentence and an empty target sequence\n",
    "    src_data = tokens\n",
    "    tgt_data = torch.tensor([tokenizer.bos_token_id]).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Generate output (prediction for next token)\n",
    "            output = model(src_data, tgt_data)\n",
    "\n",
    "            # Get the last token's logits and find the token with the highest probability\n",
    "            next_token_id = output[:, -1, :].argmax(dim=-1).item()\n",
    "\n",
    "            # Append the predicted token to the target sequence\n",
    "            tgt_data = torch.cat([tgt_data, torch.tensor([[next_token_id]]).to(device)], dim=1)\n",
    "\n",
    "            # Stop if the model outputs the [SEP] token\n",
    "            if next_token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode the token ids back to the sentence\n",
    "    translated_tokens = tgt_data.squeeze().tolist()\n",
    "    translated_sentence = tokenizer.decode(translated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return translated_sentence\n",
    "\n",
    "\n",
    "def inference_from_datasets(train_dataset: bool = True, index: int = 0):\n",
    "    if train_dataset:\n",
    "        dataset = train_loader.dataset\n",
    "    else:\n",
    "        dataset = val_loader.dataset\n",
    "    article = tokenizer.decode(dataset[index][0].tolist(), skip_special_tokens=True)\n",
    "    translation = generate_summary(article, tokenizer, transformer)\n",
    "\n",
    "    print('Dataset:', 'Train' if train_dataset else 'Validation')\n",
    "    print('Article:', article[:200])\n",
    "    print('Generated translation:', translation)\n",
    "    print('Real translation:', tokenizer.decode(dataset[index][1].tolist(), skip_special_tokens=True))\n",
    "\n",
    "\n",
    "inference_from_datasets(train_dataset=False, index=0)"
   ],
   "id": "bcce66b6b0763edf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PATH = r\"my_model_translation.pt\"\n",
    "# torch.save(transformer.state_dict(), PATH)"
   ],
   "id": "afc825fbffc59d9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# next_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n",
    "#                          max_tgt_seq_len, dropout)\n",
    "# next_model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "# next_model = next_model.to(device)\n",
    "# # print(next_model)\n",
    "#\n",
    "# # sentence = tokenizer.decode(train_loader.dataset[0][0].tolist(), skip_special_tokens=True)\n",
    "# sentence = \"What are light beans there?\"\n",
    "# print(sentence)\n",
    "# # sentence = \"Prehistoric humans studied the relationship between the seasons and the length of days to plan their hunting and gathering activities.\"\n",
    "# translation = translate_sentence(sentence, tokenizer, next_model)\n",
    "# print(translation)"
   ],
   "id": "1e2e14dd4afcbea3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "53f1646dbcb8cdcd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
