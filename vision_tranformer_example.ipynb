{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "da78b301a1d33433"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Official model",
   "id": "42235b27f740c1d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:35.911836Z",
     "start_time": "2025-02-24T16:19:33.863186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from aiohttp._websocket import mask\n",
    "\n",
    "# Check if GPU is available\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# import os\n"
   ],
   "id": "b25e86ebe92c96ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cpu\n",
      "CUDA available: False\n",
      "CUDA version: None\n",
      "GPU count: 0\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load dataset",
   "id": "a446025fac5ce2dc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:40.929818Z",
     "start_time": "2025-02-24T16:19:35.911836Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"uoft-cs/cifar10\", split='train')\n",
    "print(ds)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['img', 'label'],\n",
      "    num_rows: 50000\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare data",
   "id": "87b02503926cda8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.242161Z",
     "start_time": "2025-02-24T16:19:40.929818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "labels_map = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\",\n",
    "              8: \"ship\", 9: \"truck\"}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "images = []\n",
    "labels = []\n",
    "for i, data in enumerate(ds):\n",
    "    img_tensor = transform(data['img'])\n",
    "    images.append(img_tensor)\n",
    "    labels.append(data['label'])\n",
    "\n",
    "    if i == 500:\n",
    "        break\n",
    "# (C, H, W) --> channels, height, width. Channels are: Red, Green, Blue (each pixel can have <0-255) values in each channel. They together (mixed) provide what we see as image)\n",
    "PATCH_SIZE = 4\n",
    "print('Patch Size:', PATCH_SIZE)\n",
    "img: torch.Tensor = images[0]\n",
    "changed = img.unfold(dimension=1, size=PATCH_SIZE,\n",
    "                     step=PATCH_SIZE)  # unfold on dimension 1 so Height and we get: (Channels, New Height, Width, Patch Height)\n",
    "changed = changed.unfold(dimension=2, size=PATCH_SIZE,\n",
    "                         step=PATCH_SIZE)  # unfold on dimension 2 so Width and we get: (Channels, New Height, New Width, Patch Height, Patch Width)\n",
    "# Finally we get 3, 8, 8, 4, 4 --> channels, number of patches along height, number of patches along width, patch height, patch width\n",
    "\n",
    "# ch - channel, nph - number of patches along height, npw - number of patches along width, ph - patch height, pw - patch width\n",
    "for i in range(len(images)):\n",
    "    new_image: torch.Tensor = images[i].unfold(1, PATCH_SIZE, PATCH_SIZE).unfold(2, PATCH_SIZE,\n",
    "                                                                                 PATCH_SIZE)  # ch, nph, npw, ph, pw\n",
    "    new_image = new_image.swapdims(0, 1).swapdims(1, 2)  # nph, npw, ch, ph, pw\n",
    "    new_image = new_image.reshape(new_image.size(0) * new_image.size(1), new_image.size(2) * new_image.size(3) *\n",
    "                                  new_image.size(4))  # number of patches, all features (C * PH * PW)\n",
    "    images[i] = new_image"
   ],
   "id": "a18148b6dc1234f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Size: 4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Custom Dataset and DataLoader with train and val data (80%, 20%)",
   "id": "d5f9a69697ca4da7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.253272Z",
     "start_time": "2025-02-24T16:19:42.243909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import random_split, Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs: torch.Tensor, outputs: torch.Tensor):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "    def __setitem__(self, idx, value):\n",
    "        self.inputs[idx] = value\n",
    "\n",
    "\n",
    "dataset = CustomDataset(images, labels)\n",
    "train_data, val_data = random_split(dataset, [0.8, 0.2])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(val_data)}\")"
   ],
   "id": "482d5e998d8ec790",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 401\n",
      "Number of validation samples: 100\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.267864Z",
     "start_time": "2025-02-24T16:19:42.255288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in training set: {len(train_loader)}\")\n",
    "print(f\"Number of batches in validation set: {len(val_loader)}\")"
   ],
   "id": "cd381a10172d1af3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in training set: 26\n",
      "Number of batches in validation set: 7\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.276849Z",
     "start_time": "2025-02-24T16:19:42.269882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_dim, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.projection = nn.Linear(patch_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        return self.projection(x)\n"
   ],
   "id": "7d43f3fa15518f92",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.286622Z",
     "start_time": "2025-02-24T16:19:42.276849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CLSTokenizer(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(CLSTokenizer, self).__init__()\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        return x\n"
   ],
   "id": "8141883aeb0420f0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.304347Z",
     "start_time": "2025-02-24T16:19:42.286622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        # Initialize dimensions\n",
    "        self.d_model = d_model  # Model's dimension\n",
    "        self.num_heads = num_heads  # Number of attention heads\n",
    "        self.d_k = d_model // num_heads  # Dimension of each head's key, query, and value\n",
    "\n",
    "        # Linear layers for transforming inputs\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Output transformation\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        # Multiply by values to obtain the final output\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Apply linear transformations and split heads\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        # Perform scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Combine heads and apply output transformation\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ],
   "id": "ef607fe7d7712cd5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.323006Z",
     "start_time": "2025-02-24T16:19:42.306979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.gelu(self.fc1(x)))"
   ],
   "id": "5785cc712765f655",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.341278Z",
     "start_time": "2025-02-24T16:19:42.323006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mlp = MLP(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        residual_x = self.norm1(x)\n",
    "        attn_output = self.self_attn(residual_x, residual_x, residual_x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        residual_x = self.norm2(x)\n",
    "        mlp_output = self.mlp(residual_x)\n",
    "        x = x + self.dropout(mlp_output)\n",
    "        return x"
   ],
   "id": "67a0b8abf7ef1b80",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.357938Z",
     "start_time": "2025-02-24T16:19:42.341278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ViTModel(nn.Module):\n",
    "    def __init__(self, patch_dim, embed_dim, seq_len, num_heads, d_mlp, dropout, num_layers, num_classes):\n",
    "        super(ViTModel, self).__init__()\n",
    "        self.patch_embedding = PatchEmbedding(patch_dim, embed_dim)\n",
    "        self.cls = CLSTokenizer(embed_dim)\n",
    "        self.poss_embedding = nn.Embedding(seq_len, embed_dim)\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(embed_dim, num_heads, d_mlp, dropout) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    # def generate_mask(self, src, tgt):\n",
    "    #     src_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "    #     tgt_mask = (tgt != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(3)\n",
    "    #     src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n",
    "    #     seq_length = tgt.size(1)\n",
    "    #     casual_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "    #     casual_mask = casual_mask.to(device)\n",
    "    #     tgt_mask = tgt_mask & casual_mask\n",
    "    #     return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = x.float()\n",
    "        # 1. Patch embeddings\n",
    "        x = self.patch_embedding(x)\n",
    "        # 2. Add class token at the beginning\n",
    "        x = self.cls(x)\n",
    "        # 3. Add positional embeddings to patch embeddings (including class token)\n",
    "        pos_embedding = self.poss_embedding(torch.arange(0, x[0].size(0))).to(device)\n",
    "        x = x + pos_embedding\n",
    "        # 4. Feed encoder layers\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            x = encoder_layer(x, mask)\n",
    "        cls_output = x[:, 0]\n",
    "        output = self.fc(cls_output)\n",
    "        return output\n"
   ],
   "id": "8cfce91abab2350",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.372319Z",
     "start_time": "2025-02-24T16:19:42.359944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EMBEDDING_DIM = 128\n",
    "PATCH_DIM = images[0].size(-1)\n",
    "SEQ_LEN = images[0].size(0) + 1\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 6\n",
    "DIM_MLP = 3072\n",
    "DROPOUT = 0.1\n",
    "NUM_CLASSES = len(labels_map)"
   ],
   "id": "88b2eaebd84a3627",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:19:42.412026Z",
     "start_time": "2025-02-24T16:19:42.372319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Embedding dim:', EMBEDDING_DIM)\n",
    "print('Patch Dim:', PATCH_DIM)\n",
    "\n",
    "vitModel = ViTModel(PATCH_DIM, EMBEDDING_DIM, SEQ_LEN, NUM_HEADS, DIM_MLP, DROPOUT, NUM_LAYERS, NUM_CLASSES)\n",
    "my_model = vitModel.to(device)\n"
   ],
   "id": "2b84a8abdecef865",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dim: 128\n",
      "Patch Dim: 48\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train model",
   "id": "a977685c1564feae"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-24T16:19:42.412026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "lr = 3e-4\n",
    "optimizer = optim.AdamW(vitModel.parameters(), lr=lr, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "num_epochs = 10\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "vitModel.train()\n",
    "lr_history = []\n",
    "loss_history = []\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for src_data, tgt_data in progress_bar:\n",
    "        src_data = src_data.to(device)\n",
    "        tgt_data = tgt_data.to(device)\n",
    "\n",
    "        output = vitModel(src_data)\n",
    "\n",
    "        loss = F.cross_entropy(output, tgt_data)\n",
    "\n",
    "        lr_history.append(optimizer.param_groups[0]['lr'])\n",
    "        loss_history.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(vitModel.parameters(), 1.0)  # Prevent exploding gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {epoch_loss / len(train_loader):.4f}\")"
   ],
   "id": "4c27c273e453e232",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 26/26 [00:08<00:00,  3.17it/s, Batch Loss=2.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.5539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  31%|███       | 8/26 [00:02<00:05,  3.08it/s, Batch Loss=2.39]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Show charts with lr and loss",
   "id": "83ebbec4b3c21794"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# \n",
    "# # Ensure lr_history and loss_history are lists of equal length\n",
    "# assert len(lr_history) == len(loss_history), \"Length of lr_history and loss_history must be the same\"\n",
    "# \n",
    "# fig, ax1 = plt.subplots()\n",
    "# \n",
    "# # Plot Loss on primary y-axis\n",
    "# ax1.set_title(\"Learning Rate vs. Loss\")\n",
    "# ax1.set_xlabel(\"Training Step\")\n",
    "# ax1.set_ylabel(\"Loss\", color='tab:red')\n",
    "# ax1.plot(range(len(loss_history)), loss_history, color='tab:red', label='Loss')\n",
    "# ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "# \n",
    "# # Plot Learning Rate on secondary y-axis\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.set_ylabel(\"Learning Rate\", color='tab:blue')\n",
    "# ax2.plot(range(len(lr_history)), lr_history, color='tab:blue', linestyle='--', label='Learning Rate')\n",
    "# ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "# \n",
    "# # Legends\n",
    "# ax1.legend(loc='upper left')\n",
    "# ax2.legend(loc='upper right')\n",
    "# \n",
    "# plt.show()\n"
   ],
   "id": "d9ea000c71a19b90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate loss on validation data",
   "id": "c6f9c0e0e3f8a15d"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "vitModel.eval()\n",
    "\n",
    "total_val_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Use tqdm for progress bar\n",
    "    progress_bar = tqdm(val_loader, desc=\"Validating\")\n",
    "\n",
    "    for val_src_data, val_tgt_data in progress_bar:\n",
    "        # Move data to GPU\n",
    "        val_src_data, val_tgt_data = val_src_data.to(device), val_tgt_data.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        val_output = vitModel(val_src_data)\n",
    "\n",
    "        # Calculate loss\n",
    "        val_loss = F.cross_entropy(\n",
    "            val_output, val_tgt_data\n",
    "        )\n",
    "\n",
    "        total_val_loss += val_loss.item()\n",
    "\n",
    "        # Update progress bar with current batch loss\n",
    "        progress_bar.set_postfix({\"Batch Loss\": val_loss.item()})\n",
    "\n",
    "# Calculate average validation loss\n",
    "avg_val_loss = total_val_loss / len(val_loader)\n",
    "print(f\"Average Validation Loss: {avg_val_loss:.4f}\")"
   ],
   "id": "66ea3fdd89fb08b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model inference",
   "id": "98b42d308d6a31e2"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def classify_image(image, model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    src_data = image.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Generate output (prediction for next token)\n",
    "        output = model(src_data)\n",
    "        print('Output is:', output.shape)\n",
    "\n",
    "        # Get the last token's logits and find the token with the highest probability\n",
    "        image_class = output.argmax(dim=-1).item()\n",
    "        image_class_name = labels_map[image_class]\n",
    "\n",
    "        return image_class_name\n",
    "\n",
    "\n",
    "def inference_from_datasets(train_dataset: bool = True, index: int = 0):\n",
    "    if train_dataset:\n",
    "        dataset = train_loader.dataset\n",
    "    else:\n",
    "        dataset = val_loader.dataset\n",
    "    image = dataset[index][0]\n",
    "    print('Image is:', image.shape)\n",
    "    image_class_name = classify_image(image, model=vitModel)\n",
    "\n",
    "    print('Dataset:', 'Train' if train_dataset else 'Validation')\n",
    "    print('Generated class:', image_class_name)\n",
    "    print('Real translation:', labels_map[dataset[index][1]])\n",
    "\n",
    "\n",
    "inference_from_datasets(train_dataset=True, index=0)"
   ],
   "id": "bcce66b6b0763edf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# PATH = r\"my_model_translation.pt\"\n",
    "# torch.save(transformer.state_dict(), PATH)"
   ],
   "id": "afc825fbffc59d9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# next_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n",
    "#                          max_tgt_seq_len, dropout)\n",
    "# next_model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "# next_model = next_model.to(device)\n",
    "# # print(next_model)\n",
    "#\n",
    "# # sentence = tokenizer.decode(train_loader.dataset[0][0].tolist(), skip_special_tokens=True)\n",
    "# sentence = \"What are light beans there?\"\n",
    "# print(sentence)\n",
    "# # sentence = \"Prehistoric humans studied the relationship between the seasons and the length of days to plan their hunting and gathering activities.\"\n",
    "# translation = translate_sentence(sentence, tokenizer, next_model)\n",
    "# print(translation)"
   ],
   "id": "1e2e14dd4afcbea3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "53f1646dbcb8cdcd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
