{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "da78b301a1d33433",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "42235b27f740c1d2",
      "metadata": {},
      "source": [
        "# Official model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b25e86ebe92c96ae",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:35.911836Z",
          "start_time": "2025-02-24T16:19:33.863186Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.6.0+cpu\n",
            "CUDA available: False\n",
            "CUDA version: None\n",
            "GPU count: 0\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Check if GPU is available\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA version: {torch.version.cuda}\")\n",
        "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a446025fac5ce2dc",
      "metadata": {},
      "source": [
        "# Load dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:40.929818Z",
          "start_time": "2025-02-24T16:19:35.911836Z"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['img', 'label'],\n",
            "    num_rows: 50000\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"uoft-cs/cifar10\", split='train')\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b02503926cda8a",
      "metadata": {},
      "source": [
        "# Prepare data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a18148b6dc1234f1",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.242161Z",
          "start_time": "2025-02-24T16:19:40.929818Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patch Size: 4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "labels_map = {0: \"airplane\", 1: \"automobile\", 2: \"bird\", 3: \"cat\", 4: \"deer\", 5: \"dog\", 6: \"frog\", 7: \"horse\",\n",
        "              8: \"ship\", 9: \"truck\"}\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.PILToTensor()\n",
        "])\n",
        "images = []\n",
        "labels = []\n",
        "for i, data in enumerate(ds):\n",
        "    img_tensor = transform(data['img'])\n",
        "    images.append(img_tensor)\n",
        "    labels.append(data['label'])\n",
        "\n",
        "    if i == 500:\n",
        "        break\n",
        "# (C, H, W) --> channels, height, width. Channels are: Red, Green, Blue (each pixel can have <0-255) values in each channel. They together (mixed) provide what we see as image)\n",
        "PATCH_SIZE = 4  # Patch size 4 for images 32x32: https://arxiv.org/pdf/2210.07240 --> for small datasets\n",
        "print('Patch Size:', PATCH_SIZE)\n",
        "img: torch.Tensor = images[0]\n",
        "changed = img.unfold(dimension=1, size=PATCH_SIZE,\n",
        "                     step=PATCH_SIZE)  # unfold on dimension 1 so Height and we get: (Channels, New Height, Width, Patch Height)\n",
        "changed = changed.unfold(dimension=2, size=PATCH_SIZE,\n",
        "                         step=PATCH_SIZE)  # unfold on dimension 2 so Width and we get: (Channels, New Height, New Width, Patch Height, Patch Width)\n",
        "# Finally we get 3, 8, 8, 4, 4 --> channels, number of patches along height, number of patches along width, patch height, patch width\n",
        "\n",
        "# ch - channel, nph - number of patches along height, npw - number of patches along width, ph - patch height, pw - patch width\n",
        "for i in range(len(images)):\n",
        "    new_image: torch.Tensor = images[i].unfold(1, PATCH_SIZE, PATCH_SIZE).unfold(2, PATCH_SIZE,\n",
        "                                                                                 PATCH_SIZE)  # ch, nph, npw, ph, pw\n",
        "    new_image = new_image.swapdims(0, 1).swapdims(1, 2)  # nph, npw, ch, ph, pw\n",
        "    new_image = new_image.reshape(new_image.size(0) * new_image.size(1), new_image.size(2) * new_image.size(3) *\n",
        "                                  new_image.size(4))  # number of patches, all features (C * PH * PW)\n",
        "    images[i] = new_image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5f9a69697ca4da7",
      "metadata": {},
      "source": [
        "# Custom Dataset and DataLoader with train and val data (80%, 20%)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "482d5e998d8ec790",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.253272Z",
          "start_time": "2025-02-24T16:19:42.243909Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 401\n",
            "Number of validation samples: 100\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split, Dataset\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs: torch.Tensor, outputs: torch.Tensor):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.outputs[idx]\n",
        "\n",
        "    def __setitem__(self, idx, value):\n",
        "        self.inputs[idx] = value\n",
        "\n",
        "\n",
        "dataset = CustomDataset(images, labels)\n",
        "train_data, val_data = random_split(dataset, [0.8, 0.2])\n",
        "print(f\"Number of training samples: {len(train_data)}\")\n",
        "print(f\"Number of validation samples: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cd381a10172d1af3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.267864Z",
          "start_time": "2025-02-24T16:19:42.255288Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of batches in training set: 26\n",
            "Number of batches in validation set: 7\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"Number of batches in training set: {len(train_loader)}\")\n",
        "print(f\"Number of batches in validation set: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7d43f3fa15518f92",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.276849Z",
          "start_time": "2025-02-24T16:19:42.269882Z"
        }
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, patch_dim, embed_dim):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.projection = nn.Linear(patch_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        return self.projection(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8141883aeb0420f0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.286622Z",
          "start_time": "2025-02-24T16:19:42.276849Z"
        }
      },
      "outputs": [],
      "source": [
        "class CLSTokenizer(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(CLSTokenizer, self).__init__()\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ef607fe7d7712cd5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.304347Z",
          "start_time": "2025-02-24T16:19:42.286622Z"
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model  # Model's dimension\n",
        "        self.num_heads = num_heads  # Number of attention heads\n",
        "        self.d_k = d_model // num_heads  # Dimension of each head's key, query, and value\n",
        "\n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model)  # Query transformation\n",
        "        self.W_k = nn.Linear(d_model, d_model)  # Key transformation\n",
        "        self.W_v = nn.Linear(d_model, d_model)  # Value transformation\n",
        "        self.W_o = nn.Linear(d_model, d_model)  # Output transformation\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Multiply by values to obtain the final output\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5785cc712765f655",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.323006Z",
          "start_time": "2025-02-24T16:19:42.306979Z"
        }
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.gelu = nn.GELU(approximate='tanh')\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.gelu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "67a0b8abf7ef1b80",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.341278Z",
          "start_time": "2025-02-24T16:19:42.323006Z"
        }
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mlp = MLP(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        residual_x = self.norm1(x)\n",
        "        attn_output = self.self_attn(residual_x, residual_x, residual_x, mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        residual_x = self.norm2(x)\n",
        "        mlp_output = self.mlp(residual_x)\n",
        "        x = x + self.dropout(mlp_output)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8cfce91abab2350",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.357938Z",
          "start_time": "2025-02-24T16:19:42.341278Z"
        }
      },
      "outputs": [],
      "source": [
        "class ViTModel(nn.Module):\n",
        "    def __init__(self, patch_dim, embed_dim, seq_len, num_heads, d_mlp, dropout, num_layers, num_classes):\n",
        "        super(ViTModel, self).__init__()\n",
        "        self.patch_embedding = PatchEmbedding(patch_dim, embed_dim)\n",
        "        self.cls = CLSTokenizer(embed_dim)\n",
        "        self.poss_embedding = nn.Embedding(seq_len, embed_dim)\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(embed_dim, num_heads, d_mlp, dropout) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    # def generate_mask(self, src, tgt):\n",
        "    #     src_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
        "    #     tgt_mask = (tgt != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(3)\n",
        "    #     src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n",
        "    #     seq_length = tgt.size(1)\n",
        "    #     casual_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "    #     casual_mask = casual_mask.to(device)\n",
        "    #     tgt_mask = tgt_mask & casual_mask\n",
        "    #     return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x.float()\n",
        "        # 1. Patch embeddings\n",
        "        x = self.patch_embedding(x)\n",
        "        # 2. Add class token at the beginning\n",
        "        x = self.cls(x)\n",
        "        # 3. Add positional embeddings to patch embeddings (including class token)\n",
        "        pos_embedding = self.poss_embedding(torch.arange(0, x[0].size(0))).to(device)\n",
        "        x = x + pos_embedding\n",
        "        # 4. Feed encoder layers\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x, mask)\n",
        "        cls_output = x[:, 0]\n",
        "        output = self.fc(cls_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "88b2eaebd84a3627",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.372319Z",
          "start_time": "2025-02-24T16:19:42.359944Z"
        }
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 128\n",
        "PATCH_DIM = images[0].size(-1)\n",
        "SEQ_LEN = images[0].size(0) + 1\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 6\n",
        "DIM_MLP = 3072\n",
        "DROPOUT = 0.1\n",
        "NUM_CLASSES = len(labels_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2b84a8abdecef865",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:19:42.412026Z",
          "start_time": "2025-02-24T16:19:42.372319Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dim: 128\n",
            "Patch Dim: 48\n"
          ]
        }
      ],
      "source": [
        "print('Embedding dim:', EMBEDDING_DIM)\n",
        "print('Patch Dim:', PATCH_DIM)\n",
        "\n",
        "vitModel = ViTModel(PATCH_DIM, EMBEDDING_DIM, SEQ_LEN, NUM_HEADS, DIM_MLP, DROPOUT, NUM_LAYERS, NUM_CLASSES)\n",
        "my_model = vitModel.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a977685c1564feae",
      "metadata": {},
      "source": [
        "# Train model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4c27c273e453e232",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:31:50.515502Z",
          "start_time": "2025-02-24T16:22:50.036Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 26/26 [00:10<00:00,  2.53it/s, Batch Loss=2.4] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 1.8712\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 26/26 [00:08<00:00,  3.06it/s, Batch Loss=1.14]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2, Loss: 1.8100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 26/26 [00:07<00:00,  3.31it/s, Batch Loss=0.769]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3, Loss: 1.7772\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 26/26 [00:12<00:00,  2.15it/s, Batch Loss=0.772]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4, Loss: 1.8766\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 26/26 [00:10<00:00,  2.37it/s, Batch Loss=1.43]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5, Loss: 1.8093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 26/26 [00:10<00:00,  2.38it/s, Batch Loss=0.665]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6, Loss: 1.9425\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 26/26 [00:11<00:00,  2.30it/s, Batch Loss=1.82]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7, Loss: 1.9481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 26/26 [00:10<00:00,  2.39it/s, Batch Loss=3.6] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8, Loss: 1.9546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 26/26 [00:10<00:00,  2.37it/s, Batch Loss=0.199]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9, Loss: 1.7817\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 26/26 [00:10<00:00,  2.40it/s, Batch Loss=1.76]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10, Loss: 1.8299\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|██████████| 26/26 [00:10<00:00,  2.40it/s, Batch Loss=2.03]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11, Loss: 1.7147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|██████████| 26/26 [00:10<00:00,  2.37it/s, Batch Loss=1.66]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12, Loss: 1.6698\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|██████████| 26/26 [00:10<00:00,  2.39it/s, Batch Loss=1.69]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13, Loss: 1.5912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|██████████| 26/26 [00:13<00:00,  1.88it/s, Batch Loss=2.48]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14, Loss: 1.5347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|██████████| 26/26 [00:10<00:00,  2.38it/s, Batch Loss=0.442]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15, Loss: 1.4097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|██████████| 26/26 [00:10<00:00,  2.41it/s, Batch Loss=3.95]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16, Loss: 1.4814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|██████████| 26/26 [00:10<00:00,  2.42it/s, Batch Loss=0.903]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17, Loss: 1.2414\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|██████████| 26/26 [00:10<00:00,  2.43it/s, Batch Loss=0.133]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18, Loss: 1.1506\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|██████████| 26/26 [00:11<00:00,  2.31it/s, Batch Loss=3.55] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19, Loss: 1.0913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|██████████| 26/26 [00:10<00:00,  2.38it/s, Batch Loss=0.0464]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20, Loss: 0.8751\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|██████████| 26/26 [00:10<00:00,  2.42it/s, Batch Loss=3.68] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21, Loss: 1.0019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|██████████| 26/26 [00:10<00:00,  2.44it/s, Batch Loss=0.473]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 22, Loss: 0.7976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|██████████| 26/26 [00:10<00:00,  2.39it/s, Batch Loss=0.0539]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 23, Loss: 0.5759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|██████████| 26/26 [00:11<00:00,  2.32it/s, Batch Loss=0.44] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 24, Loss: 0.5857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|██████████| 26/26 [00:10<00:00,  2.43it/s, Batch Loss=1.25] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 25, Loss: 0.4549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|██████████| 26/26 [00:10<00:00,  2.48it/s, Batch Loss=0.124]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 26, Loss: 0.3241\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|██████████| 26/26 [00:10<00:00,  2.40it/s, Batch Loss=0.0659]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 27, Loss: 0.3323\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|██████████| 26/26 [00:10<00:00,  2.46it/s, Batch Loss=0.801] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 28, Loss: 0.3083\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|██████████| 26/26 [00:10<00:00,  2.39it/s, Batch Loss=0.000109]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 29, Loss: 0.2674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|██████████| 26/26 [00:10<00:00,  2.37it/s, Batch Loss=0.111] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 30, Loss: 0.1889\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31: 100%|██████████| 26/26 [00:10<00:00,  2.44it/s, Batch Loss=0.000989]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 31, Loss: 0.1512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32: 100%|██████████| 26/26 [00:10<00:00,  2.45it/s, Batch Loss=0.0156]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 32, Loss: 0.1277\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33: 100%|██████████| 26/26 [00:10<00:00,  2.42it/s, Batch Loss=0.0262]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 33, Loss: 0.0882\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34: 100%|██████████| 26/26 [00:10<00:00,  2.43it/s, Batch Loss=0.117] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 34, Loss: 0.0901\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35: 100%|██████████| 26/26 [00:11<00:00,  2.36it/s, Batch Loss=0.179] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 35, Loss: 0.1086\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 36: 100%|██████████| 26/26 [00:11<00:00,  2.33it/s, Batch Loss=0.0177] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 36, Loss: 0.0674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 37: 100%|██████████| 26/26 [00:10<00:00,  2.45it/s, Batch Loss=0.00143]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 37, Loss: 0.0565\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 38: 100%|██████████| 26/26 [00:10<00:00,  2.45it/s, Batch Loss=0.00057]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 38, Loss: 0.0410\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39: 100%|██████████| 26/26 [00:10<00:00,  2.44it/s, Batch Loss=0.00234]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 39, Loss: 0.0368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 40: 100%|██████████| 26/26 [00:10<00:00,  2.43it/s, Batch Loss=0.00167]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 40, Loss: 0.0112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 41: 100%|██████████| 26/26 [00:10<00:00,  2.41it/s, Batch Loss=1.56e-5] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 41, Loss: 0.0158\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 42: 100%|██████████| 26/26 [00:11<00:00,  2.34it/s, Batch Loss=0.000357]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 42, Loss: 0.0064\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 43: 100%|██████████| 26/26 [00:10<00:00,  2.40it/s, Batch Loss=6.17e-5] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 43, Loss: 0.0105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 44: 100%|██████████| 26/26 [00:11<00:00,  2.23it/s, Batch Loss=5.5e-5]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 44, Loss: 0.0023\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 45: 100%|██████████| 26/26 [00:11<00:00,  2.32it/s, Batch Loss=2.74e-5] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 45, Loss: 0.0052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 46: 100%|██████████| 26/26 [00:10<00:00,  2.48it/s, Batch Loss=0.000112]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 46, Loss: 0.0034\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 47: 100%|██████████| 26/26 [00:10<00:00,  2.39it/s, Batch Loss=8.06e-5] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 47, Loss: 0.0012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 48: 100%|██████████| 26/26 [00:10<00:00,  2.49it/s, Batch Loss=4.26e-5] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 48, Loss: 0.0010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 49: 100%|██████████| 26/26 [00:10<00:00,  2.49it/s, Batch Loss=0.000403]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 49, Loss: 0.0008\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 50: 100%|██████████| 26/26 [00:10<00:00,  2.49it/s, Batch Loss=1.18e-5] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 50, Loss: 0.0004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch import optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "lr = 3e-4\n",
        "optimizer = optim.AdamW(vitModel.parameters(), lr=lr, betas=(0.9, 0.95), eps=1e-8)\n",
        "\n",
        "num_epochs = 50\n",
        "num_training_steps = len(train_loader) * num_epochs\n",
        "num_warmup_steps = int(0.1 * num_training_steps)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "vitModel.train()\n",
        "lr_history = []\n",
        "loss_history = []\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
        "\n",
        "    for src_data, tgt_data in progress_bar:\n",
        "        src_data = src_data.to(device)\n",
        "        tgt_data = tgt_data.to(device)\n",
        "\n",
        "        output = vitModel(src_data)\n",
        "\n",
        "        loss = F.cross_entropy(output, tgt_data)\n",
        "\n",
        "        lr_history.append(optimizer.param_groups[0]['lr'])\n",
        "        loss_history.append(loss.item())\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(vitModel.parameters(), 1.0)  # Prevent exploding gradients\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update progress bar with current loss\n",
        "        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
        "    print(f\"Epoch: {epoch + 1}, Loss: {epoch_loss / len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83ebbec4b3c21794",
      "metadata": {},
      "source": [
        "# Show charts with lr and loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d9ea000c71a19b90",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:31:50.525232Z",
          "start_time": "2025-02-24T16:31:50.515502Z"
        }
      },
      "outputs": [],
      "source": [
        "# from matplotlib import pyplot as plt\n",
        "# \n",
        "# # Ensure lr_history and loss_history are lists of equal length\n",
        "# assert len(lr_history) == len(loss_history), \"Length of lr_history and loss_history must be the same\"\n",
        "# \n",
        "# fig, ax1 = plt.subplots()\n",
        "# \n",
        "# # Plot Loss on primary y-axis\n",
        "# ax1.set_title(\"Learning Rate vs. Loss\")\n",
        "# ax1.set_xlabel(\"Training Step\")\n",
        "# ax1.set_ylabel(\"Loss\", color='tab:red')\n",
        "# ax1.plot(range(len(loss_history)), loss_history, color='tab:red', label='Loss')\n",
        "# ax1.tick_params(axis='y', labelcolor='tab:red')\n",
        "# \n",
        "# # Plot Learning Rate on secondary y-axis\n",
        "# ax2 = ax1.twinx()\n",
        "# ax2.set_ylabel(\"Learning Rate\", color='tab:blue')\n",
        "# ax2.plot(range(len(lr_history)), lr_history, color='tab:blue', linestyle='--', label='Learning Rate')\n",
        "# ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
        "# \n",
        "# # Legends\n",
        "# ax1.legend(loc='upper left')\n",
        "# ax2.legend(loc='upper right')\n",
        "# \n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6f9c0e0e3f8a15d",
      "metadata": {},
      "source": [
        "# Calculate loss on validation data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "66ea3fdd89fb08b0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:38:27.267260Z",
          "start_time": "2025-02-24T16:38:26.535888Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating: 100%|██████████| 7/7 [00:00<00:00,  9.69it/s, Batch Loss=7.59]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Validation Loss: 6.4258\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "vitModel.eval()\n",
        "\n",
        "total_val_loss = 0.0\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Use tqdm for progress bar\n",
        "    progress_bar = tqdm(val_loader, desc=\"Validating\")\n",
        "\n",
        "    for val_src_data, val_tgt_data in progress_bar:\n",
        "        # Move data to GPU\n",
        "        val_src_data, val_tgt_data = val_src_data.to(device), val_tgt_data.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        val_output = vitModel(val_src_data)\n",
        "\n",
        "        # Calculate loss\n",
        "        val_loss = F.cross_entropy(\n",
        "            val_output, val_tgt_data\n",
        "        )\n",
        "\n",
        "        total_val_loss += val_loss.item()\n",
        "\n",
        "        # Update progress bar with current batch loss\n",
        "        progress_bar.set_postfix({\"Batch Loss\": val_loss.item()})\n",
        "\n",
        "# Calculate average validation loss\n",
        "avg_val_loss = total_val_loss / len(val_loader)\n",
        "print(f\"Average Validation Loss: {avg_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b42d308d6a31e2",
      "metadata": {},
      "source": [
        "# Model inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "bcce66b6b0763edf",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T17:04:05.740947Z",
          "start_time": "2025-02-24T17:04:05.687022Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image is: torch.Size([64, 48])\n",
            "Output is: torch.Size([1, 10])\n",
            "Dataset: Train\n",
            "Generated class: bird\n",
            "Real translation: bird\n"
          ]
        }
      ],
      "source": [
        "def classify_image(image, model):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    src_data = image.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate output (prediction for next token)\n",
        "        output = model(src_data)\n",
        "        print('Output is:', output.shape)\n",
        "\n",
        "        # Get the last token's logits and find the token with the highest probability\n",
        "        image_class = output.argmax(dim=-1).item()\n",
        "        image_class_name = labels_map[image_class]\n",
        "\n",
        "        return image_class_name\n",
        "\n",
        "\n",
        "def inference_from_datasets(train_dataset: bool = True, index: int = 0):\n",
        "    if train_dataset:\n",
        "        dataset = train_loader.dataset\n",
        "    else:\n",
        "        dataset = val_loader.dataset\n",
        "    image = dataset[index][0]\n",
        "    print('Image is:', image.shape)\n",
        "    image_class_name = classify_image(image, model=vitModel)\n",
        "\n",
        "    print('Dataset:', 'Train' if train_dataset else 'Validation')\n",
        "    print('Generated class:', image_class_name)\n",
        "    print('Real translation:', labels_map[dataset[index][1]])\n",
        "\n",
        "\n",
        "inference_from_datasets(train_dataset=True, index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "afc825fbffc59d9d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:21:36.461185Z",
          "start_time": "2025-02-24T16:21:36.455512Z"
        }
      },
      "outputs": [],
      "source": [
        "# PATH = r\"my_model_translation.pt\"\n",
        "# torch.save(transformer.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1e2e14dd4afcbea3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:21:36.469555Z",
          "start_time": "2025-02-24T16:21:36.463991Z"
        }
      },
      "outputs": [],
      "source": [
        "# next_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n",
        "#                          max_tgt_seq_len, dropout)\n",
        "# next_model.load_state_dict(torch.load(PATH, weights_only=True))\n",
        "# next_model = next_model.to(device)\n",
        "# # print(next_model)\n",
        "#\n",
        "# # sentence = tokenizer.decode(train_loader.dataset[0][0].tolist(), skip_special_tokens=True)\n",
        "# sentence = \"What are light beans there?\"\n",
        "# print(sentence)\n",
        "# # sentence = \"Prehistoric humans studied the relationship between the seasons and the length of days to plan their hunting and gathering activities.\"\n",
        "# translation = translate_sentence(sentence, tokenizer, next_model)\n",
        "# print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "53f1646dbcb8cdcd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-24T16:21:36.475630Z",
          "start_time": "2025-02-24T16:21:36.472573Z"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
