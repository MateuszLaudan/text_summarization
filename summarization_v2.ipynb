{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"42235b27f740c1d2","cell_type":"markdown","source":"# Summarization model","metadata":{}},{"id":"b25e86ebe92c96ae","cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\n\nseed = 42\ntorch.manual_seed(seed)  # PyTorch CPU\ntorch.cuda.manual_seed(seed)  # PyTorch GPU\ntorch.cuda.manual_seed_all(seed)  # Multi-GPU\n\n# Check if GPU is available\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"GPU count: {torch.cuda.device_count()}\")\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# device = 'cpu'\nprint(f\"Using device: {device}\")\n\n# import os\n","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:03.621293Z","start_time":"2025-02-21T19:28:01.423783Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"a446025fac5ce2dc","cell_type":"markdown","source":"# Load dataset","metadata":{}},{"id":"initial_id","cell_type":"code","source":"\nfrom datasets import load_dataset\n\nds = load_dataset(\"argilla/cnn-dailymail-summaries\", split='train', streaming=True)\nprint(ds)","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:08.018956Z","start_time":"2025-02-21T19:28:03.627297Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"87b02503926cda8a","cell_type":"markdown","source":"# Prepare data","metadata":{}},{"id":"a18148b6dc1234f1","cell_type":"code","source":"articles = []\nsummaries = []\ni = 0\nfor data in ds:\n    articles.append(data['article'])\n    summaries.append(data['highlights'])\n    i += 1\n    if i == 300:\n        break","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:08.188543Z","start_time":"2025-02-21T19:28:08.182360Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"b63c8a35c64f0a7d","cell_type":"markdown","source":"# Get Tokenizer - BartTokenizer with 50265 vocab size","metadata":{}},{"id":"bd29edde37c72d6c","cell_type":"code","source":"from transformers import BartTokenizer\n\ntokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\nVOCAB_SIZE = tokenizer.vocab_size\nSRC_MAX_SEQ = 512\nTGT_MAX_SEQ = 128\n","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:10.735280Z","start_time":"2025-02-21T19:28:08.194425Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"758e78e09478c0da","cell_type":"markdown","source":"# Clean data","metadata":{}},{"id":"13fddb44c3ac001c","cell_type":"code","source":"\"\"\"To set determined len for src and tgt --> also with pad tokens\"\"\"\n\ninputs = []\noutputs = []\nfor i in range(len(articles)):\n    article = tokenizer.encode(str(articles[i]))\n    summary = tokenizer.encode(str(summaries[i]))\n    if len(article) <= SRC_MAX_SEQ and len(summary) <= TGT_MAX_SEQ:\n        inputs.append(article)\n        outputs.append(summary)\n\ndef clean_data(sentences, max_seq_length):\n    for i, sentence in enumerate(sentences):\n        if len(sentence) < max_seq_length:\n            sentences[i] = sentence + [tokenizer.pad_token_id] * (max_seq_length - len(sentence))\n\n\nclean_data(inputs, SRC_MAX_SEQ)\nclean_data(outputs, TGT_MAX_SEQ)","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:10.860199Z","start_time":"2025-02-21T19:28:10.742073Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"b871c29a18bd72e0","cell_type":"code","source":"articles_inputs = torch.tensor(inputs)\nsummaries_outputs = torch.tensor(outputs)\nassert len(articles_inputs) == len(summaries_outputs), \"Number of articles and summaries must be the same\"\nprint('Max src seq len:', SRC_MAX_SEQ)\nprint('Max tgt seq len:', TGT_MAX_SEQ)\nprint('Number of examples:', len(inputs))\nprint(\"Number of vocab size:\", VOCAB_SIZE)\n","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:10.882097Z","start_time":"2025-02-21T19:28:10.875062Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"d5f9a69697ca4da7","cell_type":"markdown","source":"# Custom Dataset and DataLoader with train and val data (80%, 20%)","metadata":{}},{"id":"482d5e998d8ec790","cell_type":"code","source":"from torch.utils.data import random_split, Dataset\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, inputs: torch.Tensor, outputs: torch.Tensor):\n        self.inputs = inputs\n        self.outputs = outputs\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return self.inputs[idx], self.outputs[idx]\n\n\ndataset = CustomDataset(articles_inputs, summaries_outputs)\ntrain_data, val_data = random_split(dataset, [0.8, 0.2])\nprint(f\"Number of training samples: {len(train_data)}\")\nprint(f\"Number of validation samples: {len(val_data)}\")","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:10.894890Z","start_time":"2025-02-21T19:28:10.882097Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"cd381a10172d1af3","cell_type":"code","source":"from torch.utils.data import DataLoader\n\nBATCH_SIZE = 4  # TODO: expand batch size for transformer architecture\n\ntrain_loader = DataLoader(\n    train_data,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    # drop_last=True\n)\n\nval_loader = DataLoader(\n    val_data,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    # drop_last=True\n)\n\nprint(f\"Number of batches in training set: {len(train_loader)}\")\nprint(f\"Number of batches in validation set: {len(val_loader)}\")\n","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:10.915197Z","start_time":"2025-02-21T19:28:10.908208Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"ef607fe7d7712cd5","cell_type":"code","source":"import math\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads \n\n        self.W_q = nn.Linear(d_model, d_model)  # Query transformation\n        self.W_k = nn.Linear(d_model, d_model)  # Key transformation\n        self.W_v = nn.Linear(d_model, d_model)  # Value transformation\n        self.W_o = nn.Linear(d_model, d_model)  # Output transformation\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # TODO: use F..scaled_dot_product_attention\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n\n        output = torch.matmul(attn_probs, V)\n        return output\n\n    def split_heads(self, x):\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n\n    def combine_heads(self, x):\n        batch_size, _, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n\n    def forward(self, Q, K, V, mask=None):\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        output = self.W_o(self.combine_heads(attn_output))\n        return output","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:10.934307Z","start_time":"2025-02-21T19:28:10.925540Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"5785cc712765f655","cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(FeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        # Assumption: GELU outperforms ReLU which leads to 'ReLU dead neuron problem': https://arxiv.org/pdf/1606.08415\n        self.gelu = nn.GELU(approximate='tanh')\n\n    def forward(self, x):\n        return self.fc2(self.gelu(self.fc1(x)))","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:10.959635Z","start_time":"2025-02-21T19:28:10.954480Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"fe4e61c154ff8b1a","cell_type":"code","source":"\"\"\"NOTE: There is no evidence that positional encoding is better than simple learnable embeddings.\"\"\"\n\n#\n# class PositionalEncoding(nn.Module):\n#     def __init__(self, d_model, max_seq_length):\n#         super(PositionalEncoding, self).__init__()\n#\n#         pe = torch.zeros(max_seq_length, d_model)\n#         position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n#\n#         pe[:, 0::2] = torch.sin(position * div_term)\n#         pe[:, 1::2] = torch.cos(position * div_term)\n#\n#         self.register_buffer('pe', pe.unsqueeze(0))\n#\n#     def forward(self, x):\n#         return x + self.pe[:, :x.size(1)]","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:10.971071Z","start_time":"2025-02-21T19:28:10.959635Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"67a0b8abf7ef1b80","cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)  # TODO: one dropout layer or two?\n\n    def forward(self, x, mask):\n        # Normalization before sub-blocks: as described at: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n        residual_x = self.norm1(x)  # residual_x described at: https://arxiv.org/pdf/1904.10509\n        attn_output = self.self_attn(residual_x, residual_x, residual_x, mask)\n        x = x + self.dropout(attn_output)\n        residual_x = self.norm2(x)  # Normalization before sub-blocks\n        ff_output = self.feed_forward(residual_x)\n        x = x + self.dropout(ff_output)\n        return x","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:10.986469Z","start_time":"2025-02-21T19:28:10.980972Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"13f775b689e91e6e","cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        # Normalization before sub-blocks: as described at: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n        residual_x = self.norm1(x)\n        attn_output = self.self_attn(residual_x, residual_x, residual_x, tgt_mask)\n        x = x + self.dropout(attn_output)\n        residual_x = self.norm2(x)  # Normalization before sub-blocks\n        attn_output = self.cross_attn(residual_x, enc_output, enc_output, src_mask)\n        x = x + self.dropout(attn_output)\n        residual_x = self.norm3(x)  # Normalization before sub-blocks\n        ff_output = self.feed_forward(residual_x)\n        x = x + self.dropout(ff_output)\n        return x","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:11.003505Z","start_time":"2025-02-21T19:28:10.997164Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"1f6b502c5537500e","cell_type":"code","source":"import sys\n\n\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n                 max_tgt_seq_len, dropout):\n        super(Transformer, self).__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        # self.src_positional_encoding = PositionalEncoding(d_model, max_src_seq_len)\n        # self.tgt_positional_encoding = PositionalEncoding(d_model, max_tgt_seq_len)\n        self.src_positional_encoding = nn.Embedding(max_src_seq_len,\n                                                    d_model)  # instead of cos and sin functions (in PositionalEncoding)\n        self.tgt_positional_encoding = nn.Embedding(max_tgt_seq_len, d_model)\n\n        self.encoder_layers = nn.ModuleList(\n            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList(\n            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n        # One more LayerNorm as described: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n        self.ln_f = nn.LayerNorm(d_model)\n        self.output_layer = nn.Linear(d_model, tgt_vocab_size,\n                                      bias=False)  # bias=False as depicted: https://github.com/karpathy/nanoGPT/blob/master/model.py\n        self.dropout = nn.Dropout(dropout)\n\n    def generate_mask(self, src, tgt):\n        src_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n        tgt_mask = (tgt != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(3)\n        src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n        seq_length = tgt.size(1)\n        casual_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n        casual_mask = casual_mask.to(device)\n        tgt_mask = tgt_mask & casual_mask\n        return src_mask, tgt_mask\n\n    def forward(self, src, tgt):\n        src_mask, tgt_mask = self.generate_mask(src, tgt)\n        # src_embedded = self.dropout(self.src_positional_encoding(self.encoder_embedding(src)))\n        # tgt_embedded = self.dropout(self.tgt_positional_encoding(self.decoder_embedding(tgt)))\n\n        src_pos = torch.arange(0, src.size(1)).to(device)  # [0, 1, 2 ... src.size(1)]\n        src_pos = self.src_positional_encoding(src_pos)\n        src_embedded = self.encoder_embedding(src)\n        src_embedded = self.dropout(src_pos + src_embedded)\n\n        tgt_pos = torch.arange(0, tgt.size(1)).to(device)\n        tgt_pos = self.tgt_positional_encoding(tgt_pos)\n        tgt_embedded = self.decoder_embedding(tgt)\n        tgt_embedded = self.dropout(tgt_pos + tgt_embedded)\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n\n        # One more LayerNorm as described: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n        dec_output = self.ln_f(dec_output)\n        output = self.output_layer(dec_output)\n        return output","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:11.023472Z","start_time":"2025-02-21T19:28:11.013760Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"88b2eaebd84a3627","cell_type":"code","source":"src_vocab_size = VOCAB_SIZE\ntgt_vocab_size = VOCAB_SIZE\nd_model = 512  # TODO: experiment with model complexity - it can lead to overfitting\nnum_heads = 8\nnum_layers = 6  # TODO: how many layers? as above, model complexity\nd_ff = d_model * 4\nmax_src_seq_len = SRC_MAX_SEQ\nmax_tgt_seq_len = TGT_MAX_SEQ\ndropout = 0.2","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:11.038543Z","start_time":"2025-02-21T19:28:11.033477Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"930121124b5e2e91","cell_type":"code","source":"transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n                          max_tgt_seq_len,\n                          dropout)\ntransformer.to(device)\nnum_parameters = sum(p.numel() for p in transformer.parameters())\nprint(f\"Number of parameters: {num_parameters/1000000} M\")\n","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:11.855642Z","start_time":"2025-02-21T19:28:11.050783Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"a977685c1564feae","cell_type":"markdown","source":"# Train model","metadata":{}},{"id":"4c27c273e453e232","cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch import optim\nfrom tqdm import tqdm\n\ntorch.manual_seed(seed) \ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\nlr = 0.0005\noptimizer = optim.Adam(\n    transformer.parameters(), lr=lr, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.0001\n)\n\n# TODO: experiment with gradient accumulation\nnum_epochs = 10\nnum_training_steps = len(train_loader) * num_epochs\nnum_warmup_steps = int(0.1 * num_training_steps) # TODO: experiment with that\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n)\n\n# Tracking history\nlr_history = []\nloss_history = []\nval_loss_history = []\n\nfor epoch in range(num_epochs):\n    epoch_loss = 0.0\n    transformer.train()\n\n    # Progress bar for training\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n\n    for step, (src_data, tgt_data) in enumerate(progress_bar):\n        src_data, tgt_data = src_data.to(device), tgt_data.to(device)\n\n        # Forward pass\n        output = transformer(src_data, tgt_data[:, :-1])\n\n        # Compute loss\n        loss = F.cross_entropy(\n            output.contiguous().view(-1, tgt_vocab_size),\n            tgt_data[:, 1:].contiguous().view(-1),\n            ignore_index=tokenizer.pad_token_id\n        )\n\n        # Track loss\n        loss_history.append(loss.item())\n        epoch_loss += loss.item()\n        lr_history.append(optimizer.param_groups[0][\"lr\"])\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\ns\n        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n\n    avg_epoch_loss = epoch_loss / len(train_loader)\n    print(f\"Epoch: {epoch + 1}, Loss: {avg_epoch_loss:.4f}\")\n\n    transformer.eval()\n    total_val_loss = 0.0\n\n    with torch.no_grad():\n        for val_src_data, val_tgt_data in val_loader:\n            val_src_data, val_tgt_data = val_src_data.to(device), val_tgt_data.to(device)\n\n            # Forward pass\n            val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n\n            val_loss = F.cross_entropy(\n                val_output.contiguous().view(-1, tgt_vocab_size),\n                val_tgt_data[:, 1:].contiguous().view(-1),\n                ignore_index=tokenizer.pad_token_id\n            )\n\n            total_val_loss += val_loss.item()\n\n    # Calculate average validation loss\n    avg_val_loss = total_val_loss / len(val_loader)\n    val_loss_history.append(avg_val_loss)\n    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:37:44.972872Z","start_time":"2025-02-21T19:36:32.220934Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"83ebbec4b3c21794","cell_type":"markdown","source":"# Show charts with lr and loss","metadata":{}},{"id":"d9ea000c71a19b90","cell_type":"code","source":"from matplotlib import pyplot as plt\n\n# Ensure all histories are properly formatted\nassert len(lr_history) == len(\n    loss_history\n), \"Length of lr_history and loss_history must be the same\"\n\n# Create figure and primary y-axis for Loss\nfig, ax1 = plt.subplots()\n\nax1.set_title(\"Learning Rate vs. Loss\")\nax1.set_xlabel(\"Training Step\")\nax1.set_ylabel(\"Loss\", color=\"tab:red\")\nax1.plot(range(len(loss_history)), loss_history, color=\"tab:red\", label=\"Training Loss\")\nax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n\n# Plot validation loss (assuming it occurs every 'epoch_interval' steps)\nepoch_interval = len(loss_history) // len(val_loss_history)\nval_x = [\n    i * epoch_interval for i in range(len(val_loss_history))\n]  # X values for validation loss\nax1.plot(\n    val_x,\n    val_loss_history,\n    color=\"tab:orange\",\n    marker=\"o\",\n    linestyle=\"dashed\",\n    label=\"Validation Loss\",\n)\n\n# Create secondary y-axis for Learning Rate\nax2 = ax1.twinx()\nax2.set_ylabel(\"Learning Rate\", color=\"tab:blue\")\nax2.plot(\n    range(len(lr_history)),\n    lr_history,\n    color=\"tab:blue\",\n    linestyle=\"--\",\n    label=\"Learning Rate\",\n)\nax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n\n# Legends\nax1.legend(loc=\"upper left\")\nax2.legend(loc=\"upper right\")\n\nplt.show()","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:42:03.670352Z","start_time":"2025-02-21T19:42:03.439490Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"98b42d308d6a31e2","cell_type":"markdown","source":"# Model inference","metadata":{}},{"id":"bcce66b6b0763edf","cell_type":"code","source":"def generate_summary(sentence, tokenizer, model, max_length=TGT_MAX_SEQ):\n    \"\"\"\n    Translates a single Polish sentence into Ukrainian using greedy decoding.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n\n    tokens = tokenizer.encode(sentence)\n    tokens = torch.tensor(tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n    # print(tokens)\n\n    # Start with the input sentence and an empty target sequence\n    src_data = tokens\n    tgt_data = torch.tensor([tokenizer.bos_token_id]).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        for _ in range(max_length):\n            output = model(src_data, tgt_data)\n\n            next_token_id = output[:, -1, :].argmax(dim=-1).item()\n\n            tgt_data = torch.cat([tgt_data, torch.tensor([[next_token_id]]).to(device)], dim=1)\n\n            if next_token_id == tokenizer.eos_token_id:\n                break\n\n    # Decode the token ids back to the sentence\n    translated_tokens = tgt_data.squeeze().tolist()\n    translated_sentence = tokenizer.decode(translated_tokens, skip_special_tokens=True)\n\n    return translated_sentence\n\n\ndef inference_from_datasets(train_dataset: bool = True, index: int = 0):\n    if train_dataset:\n        dataset = train_loader.dataset\n    else:\n        dataset = val_loader.dataset\n    article = tokenizer.decode(dataset[index][0].tolist(), skip_special_tokens=True)\n    translation = generate_summary(article, tokenizer, transformer)\n\n    print('Dataset:', 'Train' if train_dataset else 'Validation')\n    print('Article:', article[:200])\n    print('Generated translation:', translation)\n    print('Real translation:', tokenizer.decode(dataset[index][1].tolist(), skip_special_tokens=True))\n\n\ninference_from_datasets(train_dataset=True, index=0)","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:38.653718Z","start_time":"2025-02-21T19:28:37.673898Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"afc825fbffc59d9d","cell_type":"code","source":"# PATH = r\"my_model_translation.pt\"\n# torch.save(transformer.state_dict(), PATH)","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:15.830404Z","start_time":"2025-02-21T19:28:15.825327Z"},"trusted":true},"outputs":[],"execution_count":null},{"id":"1e2e14dd4afcbea3","cell_type":"code","source":"# next_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n#                          max_tgt_seq_len, dropout)\n# next_model.load_state_dict(torch.load(PATH, weights_only=True))\n# next_model = next_model.to(device)\n# # print(next_model)\n#\n# # sentence = tokenizer.decode(train_loader.dataset[0][0].tolist(), skip_special_tokens=True)\n# sentence = \"What are light beans there?\"\n# print(sentence)\n# # sentence = \"Prehistoric humans studied the relationship between the seasons and the length of days to plan their hunting and gathering activities.\"\n# translation = translate_sentence(sentence, tokenizer, next_model)\n# print(translation)","metadata":{"ExecuteTime":{"end_time":"2025-02-21T19:28:15.898792Z","start_time":"2025-02-21T19:28:15.894203Z"},"trusted":true},"outputs":[],"execution_count":null}]}