{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42235b27f740c1d2",
   "metadata": {},
   "source": [
    "<!-- # Summarization model -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e86ebe92c96ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:00:53.535200Z",
     "start_time": "2025-03-14T14:00:51.569674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)  # PyTorch CPU\n",
    "torch.cuda.manual_seed(seed)  # PyTorch GPU\n",
    "torch.cuda.manual_seed_all(seed)  # Multi-GPU\n",
    "\n",
    "# Check if GPU is available\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a446025fac5ce2dc",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:00:57.360280Z",
     "start_time": "2025-03-14T14:00:53.535200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"FrancophonIA/french-to-english\", split=\"train\", streaming=True)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c8a35c64f0a7d",
   "metadata": {},
   "source": [
    "# Get Tokenizer - BartTokenizer with 50265 vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29edde37c72d6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:01:05.976725Z",
     "start_time": "2025-03-14T14:01:03.601837Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "SRC_MAX_SEQ = 50\n",
    "TGT_MAX_SEQ = 50\n",
    "max_examples = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b02503926cda8a",
   "metadata": {},
   "source": [
    "# Prepare data - save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9947aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\n",
    "pipe = pipeline(\"text-classification\", model=model_ckpt, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18148b6dc1234f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:01:03.594626Z",
     "start_time": "2025-03-14T14:00:57.522389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# inputs = []\n",
    "# outputs = []\n",
    "# i = 0\n",
    "# for data in ds:\n",
    "#     src_sentence = tokenizer.encode(str(data['en']))\n",
    "#     tgt_sentence = tokenizer.encode(str(data['fr']))\n",
    "#     if len(src_sentence) <= SRC_MAX_SEQ and len(tgt_sentence) <= TGT_MAX_SEQ:\n",
    "#         try:\n",
    "#             if not data.get('fr') or not isinstance(data['fr'], str):\n",
    "#                 continue  # Skip invalid entries\n",
    "#             else:\n",
    "#                 res = pipe(data['fr'], top_k=1, truncation=True)[0]['label']\n",
    "#                 if res == 'fr':       \n",
    "#                     inputs.append(src_sentence)\n",
    "#                     outputs.append(tgt_sentence)\n",
    "#                     i += 1\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error {e}\")\n",
    "#             continue       \n",
    "#     if i == max_examples:\n",
    "#         break\n",
    "# # Save as JSON\n",
    "# with open('tokenized_data.json', 'w') as f:\n",
    "#     json.dump({'inputs': inputs, 'outputs': outputs}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082ee82",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "\n",
    "with open('tokenized_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    inputs = data['inputs'][:max_examples]\n",
    "    outputs = data['outputs'][:max_examples]\n",
    "print(len(inputs), len(outputs))\n",
    "# for inp, out in zip(inputs, outputs):\n",
    "#     out = tokenizer.decode(out, skip_special_tokens=True)\n",
    "#     inp = tokenizer.decode(inp, skip_special_tokens=True)\n",
    "#     print(inp)\n",
    "#     print(out)\n",
    "#     print()\n",
    "#     res = pipe(out, top_k=1, truncation=True)[0]['label']\n",
    "#     if res != 'fr':\n",
    "#         print(res, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e78e09478c0da",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fddb44c3ac001c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.204862Z",
     "start_time": "2025-03-14T14:01:05.982943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"To set determined len for src and tgt --> also with pad tokens\"\"\"\n",
    "\n",
    "\n",
    "def clean_data(sentences, max_seq_length):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(sentence) < max_seq_length:\n",
    "            sentences[i] = sentence + [tokenizer.pad_token_id] * (max_seq_length - len(sentence))\n",
    "\n",
    "\n",
    "clean_data(inputs, SRC_MAX_SEQ)\n",
    "clean_data(outputs, TGT_MAX_SEQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a144e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (inp, out) in enumerate(zip(inputs, outputs)):\n",
    "    print(tokenizer.decode(inp, skip_special_tokens=True), '######', tokenizer.decode(out, skip_special_tokens=True))\n",
    "    if step == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871c29a18bd72e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.786438Z",
     "start_time": "2025-03-14T14:02:36.229746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(outputs)\n",
    "assert len(inputs) == len(targets), \"Number of articles and summaries must be the same\"\n",
    "print('Max src seq len:', SRC_MAX_SEQ)\n",
    "print('Max tgt seq len:', TGT_MAX_SEQ)\n",
    "print('Number of examples:', len(inputs))\n",
    "print(\"Number of vocab size:\", VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f9a69697ca4da7",
   "metadata": {},
   "source": [
    "# Custom Dataset and DataLoader with train and val data (80%, 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d5e998d8ec790",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.816089Z",
     "start_time": "2025-03-14T14:02:36.800441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs: torch.Tensor, outputs: torch.Tensor):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "\n",
    "dataset = CustomDataset(inputs, targets)\n",
    "train_data, val_data = random_split(dataset, [0.8, 0.2])\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of validation samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd381a10172d1af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.832311Z",
     "start_time": "2025-03-14T14:02:36.825793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 128  # TODO: expand batch size for transformer architecture\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    # drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    # drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in training set: {len(train_loader)}\")\n",
    "print(f\"Number of batches in validation set: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a292d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(train_loader.dataset[0][0], skip_special_tokens=True) )\n",
    "print(tokenizer.decode(train_loader.dataset[0][1], skip_special_tokens=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef607fe7d7712cd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.875983Z",
     "start_time": "2025-03-14T14:02:36.841289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads \n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Query transformation\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # Key transformation\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Value transformation\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Output transformation\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # TODO: use F..scaled_dot_product_attention\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5785cc712765f655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.889939Z",
     "start_time": "2025-03-14T14:02:36.885027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        # Assumption: GELU outperforms ReLU which leads to 'ReLU dead neuron problem': https://arxiv.org/pdf/1606.08415\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e61c154ff8b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.905657Z",
     "start_time": "2025-03-14T14:02:36.898280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"NOTE: There is no evidence that positional encoding is better than simple learnable embeddings.\"\"\"\n",
    "\n",
    "#\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_seq_length):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#\n",
    "#         pe = torch.zeros(max_seq_length, d_model)\n",
    "#         position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "#\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#\n",
    "#         self.register_buffer('pe', pe.unsqueeze(0))\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0b8abf7ef1b80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.920098Z",
     "start_time": "2025-03-14T14:02:36.914083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)  # TODO: one dropout layer or two?\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Normalization before sub-blocks: as described at: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "        residual_x = self.norm1(x)  # residual_x described at: https://arxiv.org/pdf/1904.10509\n",
    "        attn_output = self.self_attn(residual_x, residual_x, residual_x, mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        residual_x = self.norm2(x)  # Normalization before sub-blocks\n",
    "        ff_output = self.feed_forward(residual_x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f775b689e91e6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.935223Z",
     "start_time": "2025-03-14T14:02:36.929678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        # Normalization before sub-blocks: as described at: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "        residual_x = self.norm1(x)\n",
    "        attn_output = self.self_attn(residual_x, residual_x, residual_x, tgt_mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        residual_x = self.norm2(x)  # Normalization before sub-blocks\n",
    "        attn_output = self.cross_attn(residual_x, enc_output, enc_output, src_mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        residual_x = self.norm3(x)  # Normalization before sub-blocks\n",
    "        ff_output = self.feed_forward(residual_x)\n",
    "        x = x + self.dropout(ff_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b502c5537500e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.976478Z",
     "start_time": "2025-03-14T14:02:36.947631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n",
    "                 max_tgt_seq_len, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        # self.src_positional_encoding = PositionalEncoding(d_model, max_src_seq_len)\n",
    "        # self.tgt_positional_encoding = PositionalEncoding(d_model, max_tgt_seq_len)\n",
    "        self.src_positional_encoding = nn.Embedding(max_src_seq_len,\n",
    "                                                    d_model)  # instead of cos and sin functions (in PositionalEncoding)\n",
    "        self.tgt_positional_encoding = nn.Embedding(max_tgt_seq_len, d_model)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # One more LayerNorm as described: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.output_layer = nn.Linear(d_model, tgt_vocab_size,\n",
    "                                      bias=False)  # bias=False as depicted: https://github.com/karpathy/nanoGPT/blob/master/model.py\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != tokenizer.pad_token_id).unsqueeze(1).unsqueeze(3)\n",
    "        src_mask, tgt_mask = src_mask.to(device), tgt_mask.to(device)\n",
    "        seq_length = tgt.size(1)\n",
    "        casual_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        casual_mask = casual_mask.to(device)\n",
    "        tgt_mask = tgt_mask & casual_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        # src_embedded = self.dropout(self.src_positional_encoding(self.encoder_embedding(src)))\n",
    "        # tgt_embedded = self.dropout(self.tgt_positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        src_pos = torch.arange(0, src.size(1)).to(device)  # [0, 1, 2 ... src.size(1)]\n",
    "        src_pos = self.src_positional_encoding(src_pos)\n",
    "        src_embedded = self.encoder_embedding(src)\n",
    "        src_embedded = self.dropout(src_pos + src_embedded)\n",
    "\n",
    "        tgt_pos = torch.arange(0, tgt.size(1)).to(device)\n",
    "        tgt_pos = self.tgt_positional_encoding(tgt_pos)\n",
    "        tgt_embedded = self.decoder_embedding(tgt)\n",
    "        tgt_embedded = self.dropout(tgt_pos + tgt_embedded)\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        # One more LayerNorm as described: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "        dec_output = self.ln_f(dec_output)\n",
    "        output = self.output_layer(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2eaebd84a3627",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:36.990503Z",
     "start_time": "2025-03-14T14:02:36.986534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "src_vocab_size = VOCAB_SIZE\n",
    "tgt_vocab_size = VOCAB_SIZE\n",
    "d_model = 128  # TODO: experiment with model complexity - it can lead to overfitting\n",
    "num_heads = 1  # TODO: layers + heads have impact on final results: https://medium.com/@ccibeekeoc42/unveiling-the-transformer-impact-of-layers-and-attention-heads-in-audio-classification-58747d52b794\n",
    "num_layers = 6  # TODO: how many layers? as above, model complexity; smaller BART uses 6: https://arxiv.org/pdf/1910.13461\n",
    "d_ff = d_model * 4\n",
    "max_src_seq_len = SRC_MAX_SEQ\n",
    "max_tgt_seq_len = TGT_MAX_SEQ\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930121124b5e2e91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:02:37.383510Z",
     "start_time": "2025-03-14T14:02:36.999834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n",
    "                          max_tgt_seq_len,\n",
    "                          dropout)\n",
    "transformer.to(device)\n",
    "num_parameters = sum(p.numel() for p in transformer.parameters())\n",
    "print(f\"Number of parameters: {num_parameters/1000000} M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977685c1564feae",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c27c273e453e232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:05:04.407645Z",
     "start_time": "2025-03-14T14:02:37.394025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(seed) \n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "lr = 0.0007\n",
    "optimizer = optim.Adam(\n",
    "    transformer.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.00\n",
    ")\n",
    "\n",
    "# TODO: experiment with gradient accumulation\n",
    "num_epochs = 1\n",
    "# TODO: add warmup steps\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "num_warmup_steps = int(0.1 * num_training_steps) # TODO: experiment with that\n",
    " \n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Tracking history\n",
    "lr_history = []\n",
    "loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    transformer.train()\n",
    "\n",
    "    # Progress bar for training\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for step, (src_data, tgt_data) in enumerate(progress_bar):\n",
    "        src_data, tgt_data = src_data.to(device), tgt_data.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = transformer(src_data, tgt_data[:, :-1])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(\n",
    "            output.contiguous().view(-1, tgt_vocab_size),\n",
    "            tgt_data[:, 1:].contiguous().view(-1),\n",
    "            ignore_index=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "        # Track loss\n",
    "        loss_history.append(loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "        lr_history.append(optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        progress_bar.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    transformer.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_src_data, val_tgt_data in val_loader:\n",
    "            val_src_data, val_tgt_data = val_src_data.to(device), val_tgt_data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "\n",
    "            val_loss = F.cross_entropy(\n",
    "                val_output.contiguous().view(-1, tgt_vocab_size),\n",
    "                val_tgt_data[:, 1:].contiguous().view(-1),\n",
    "                ignore_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    # Calculate average validation loss\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del transformer\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ebbec4b3c21794",
   "metadata": {},
   "source": [
    "# Show charts with lr and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea000c71a19b90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:05:08.610197Z",
     "start_time": "2025-03-14T14:05:08.441158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Ensure all histories are properly formatted\n",
    "assert len(lr_history) == len(\n",
    "    loss_history\n",
    "), \"Length of lr_history and loss_history must be the same\"\n",
    "\n",
    "# Create figure and primary y-axis for Loss\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_title(\"Learning Rate vs. Loss\")\n",
    "ax1.set_xlabel(\"Training Step\")\n",
    "ax1.set_ylabel(\"Loss\", color=\"tab:red\")\n",
    "ax1.plot(range(len(loss_history)), loss_history, color=\"tab:red\", label=\"Training Loss\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "# Plot validation loss (assuming it occurs every 'epoch_interval' steps)\n",
    "epoch_interval = len(loss_history) // len(val_loss_history)\n",
    "val_x = [\n",
    "    i * epoch_interval for i in range(len(val_loss_history))\n",
    "]  # X values for validation loss\n",
    "ax1.plot(\n",
    "    val_x,\n",
    "    val_loss_history,\n",
    "    color=\"tab:orange\",\n",
    "    marker=\"o\",\n",
    "    linestyle=\"dashed\",\n",
    "    label=\"Validation Loss\",\n",
    ")\n",
    "\n",
    "# Create secondary y-axis for Learning Rate\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Learning Rate\", color=\"tab:blue\")\n",
    "ax2.plot(\n",
    "    range(len(lr_history)),\n",
    "    lr_history,\n",
    "    color=\"tab:blue\",\n",
    "    linestyle=\"--\",\n",
    "    label=\"Learning Rate\",\n",
    ")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "# Legends\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b42d308d6a31e2",
   "metadata": {},
   "source": [
    "# Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcce66b6b0763edf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:05:18.725518Z",
     "start_time": "2025-03-14T14:05:17.948403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inference(input, tokenizer, model, max_length=TGT_MAX_SEQ):\n",
    "    \"\"\"\n",
    "    Translates a single Polish sentence into Ukrainian using greedy decoding.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    tokens = tokenizer.encode(input)\n",
    "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)  # Shape: (1, seq_len)\n",
    "    # print(tokens)\n",
    "\n",
    "    # Start with the input sentence and an empty target sequence\n",
    "    src_data = tokens\n",
    "    tgt_data = torch.tensor([tokenizer.bos_token_id]).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output = model(src_data, tgt_data)\n",
    "\n",
    "            next_token_id = output[:, -1, :].argmax(dim=-1).item()\n",
    "\n",
    "            tgt_data = torch.cat([tgt_data, torch.tensor([[next_token_id]]).to(device)], dim=1)\n",
    "\n",
    "            if next_token_id == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Decode the token ids back to the sentence\n",
    "    translated_tokens = tgt_data.squeeze().tolist()\n",
    "    translated_sentence = tokenizer.decode(translated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return translated_sentence\n",
    "\n",
    "\n",
    "def inference_from_datasets(train_dataset: bool = True, index: int = 0) -> (str, str):\n",
    "    if train_dataset:\n",
    "        dataset = train_loader.dataset\n",
    "    else:\n",
    "        dataset = val_loader.dataset\n",
    "    src_input = tokenizer.decode(dataset[index][0].tolist(), skip_special_tokens=True)\n",
    "    translation = inference(src_input, tokenizer, transformer)\n",
    "\n",
    "    print('Dataset:', 'Train' if train_dataset else 'Validation')\n",
    "    print('Src:', src_input)\n",
    "    print('Generated translation:', translation)\n",
    "    real_translation = tokenizer.decode(dataset[index][1].tolist(), skip_special_tokens=True)\n",
    "    print('Real translation:      ', real_translation)\n",
    "    return src_input, translation, real_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c365308",
   "metadata": {},
   "source": [
    "<!-- # SacreBLEU metrics -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import httpx\n",
    "# Load metrics\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "chrf = evaluate.load(\"chrf\")\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b5ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load variables from .env\n",
    "\n",
    "deepl_auth_key = os.getenv(\"DEEPL_AUTH_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8feb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Docs: https://huggingface.co/spaces/evaluate-metric/sacrebleu\n",
    "# score from 0 to 100\n",
    "local_accuracy = 0.0\n",
    "deepl_accuracy = 0.0\n",
    "\n",
    "def compute(predictions, references):\n",
    "    # Predictions and references\n",
    "    predictions = [predictions]\n",
    "    references = [[references]]\n",
    "\n",
    "    # Compute all metrics\n",
    "    results_rouge = rouge.compute(predictions=predictions, references=references)  # ROUGE expects list of strings\n",
    "    results_sacrebleu = sacrebleu.compute(predictions=predictions, references=references)\n",
    "    results_meteor = meteor.compute(predictions=predictions, references=references)\n",
    "    results_chrf = chrf.compute(predictions=predictions, references=references)\n",
    "    results_bleu = bleu.compute(predictions=predictions, references=references)\n",
    "    return results_rouge['rouge1']\n",
    "\n",
    "    # Print results\n",
    "    # print(\"\\nðŸ“Š Translation Metrics:\")\n",
    "    # print(f\"ROUGE-1 (0â€“1):        {results_rouge['rouge1']:.4f}\")  # Overlap of unigrams (single words) between the generated and reference texts.\n",
    "    # print(f\"ROUGE-2 (0â€“1):        {results_rouge['rouge2']:.4f}\")  # Overlap of bigrams (word pairs).\n",
    "    # print(f\"ROUGE-L (0â€“1):        {results_rouge['rougeL']:.4f}\")  # Measures longest common subsequence (sequence similarity).\n",
    "    # print(f\"SacreBLEU (0â€“100):    {results_sacrebleu['score']:.2f}\")  # Precision-based score for how many matching words/phrases, adjusted for brevity.\n",
    "    # print(f\"METEOR (0â€“1):         {results_meteor['meteor']:.4f}\")  #  Considers word matches, stemming, and synonyms with penalties for word order.\n",
    "    # print(f\"chrF (0â€“100):         {results_chrf['score']:.2f}\")  # Measures character-level n-gram overlap (more sensitive to small variations).\n",
    "    # print(f\"BLEU (0â€“100):         {results_bleu['bleu']:.2f}\")  # Measures n-gram overlap between generated and reference texts.\n",
    "\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    src_input, predictions, references = inference_from_datasets(train_dataset=False, index=i)  # translation, real_translation\n",
    "\n",
    "\n",
    "    json_data = {\n",
    "        \"text\": [src_input], \n",
    "        \"target_lang\": \"FR\"\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"DeepL-Auth-Key {deepl_auth_key}\"}\n",
    "    response = httpx.post(\"https://api-free.deepl.com/v2/translate\", json=json_data, headers=headers)\n",
    "    response = response.json()[\"translations\"][0][\"text\"]\n",
    "    print('Translation from DeepL:', response)\n",
    "\n",
    "    local_accuracy += compute(predictions, references)\n",
    "    print()\n",
    "    deepl_accuracy += compute(predictions, response)\n",
    "\n",
    "print('Local accuracy:', local_accuracy/n)\n",
    "print('DeepL accuracy:', deepl_accuracy/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf81597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc825fbffc59d9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:05:05.855584Z",
     "start_time": "2025-03-14T14:05:05.850821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PATH = r\"new_translation_model.pt\"\n",
    "# torch.save(transformer.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e14dd4afcbea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T14:05:05.944091Z",
     "start_time": "2025-03-14T14:05:05.940396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# next_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_src_seq_len,\n",
    "#                          max_tgt_seq_len, dropout)\n",
    "# next_model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "# next_model = next_model.to(device)\n",
    "# # print(next_model)\n",
    "\n",
    "# # sentence = tokenizer.decode(train_loader.dataset[0][0].tolist(), skip_special_tokens=True)\n",
    "# sentence = \"What are light beans there?\"\n",
    "# print(sentence)\n",
    "# # sentence = \"Prehistoric humans studied the relationship between the seasons and the length of days to plan their hunting and gathering activities.\"\n",
    "# translation = translate_sentence(sentence, tokenizer, next_model)\n",
    "# print(translation)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
