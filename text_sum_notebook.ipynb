{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import inspect\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if GPU is available\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, VerificationMode\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Load the entire dataset\n",
    "# dataset = load_dataset(\"...\", verification_mode=VerificationMode.NO_CHECKS)\n",
    "dataset = load_dataset(\"argilla/cnn-dailymail-summaries\", split='train', streaming=True)\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "i = 0\n",
    "max = 3\n",
    "\n",
    "news = []\n",
    "summaries = []\n",
    "for d in dataset:\n",
    "  i += 1\n",
    "  news.append(d['article'])\n",
    "  summaries.append(d['highlights'])\n",
    "  if i == max:\n",
    "    break\n",
    "print(f'Number of news: {len(news)}')\n",
    "print(f'Number of summaries: {len(summaries)}')"
   ],
   "id": "63a134c06afa03ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"Tokens <s> and </s> are added from default in BartTokenizer\"\"\"\n",
    "\n",
    "# Padding was requried due to different size of inputs (each article is different): https://huggingface.co/docs/transformers/en/pad_truncation\n",
    "inputs = tokenizer(news, padding='longest', truncation=True, max_length=1024)['input_ids']\n",
    "targets = tokenizer(summaries, padding='longest', truncation=True, max_length=128)['input_ids']  # --> padding to the longest text but shorter or equal max_len (128)\n",
    "print(tokenizer.decode(inputs[0]))\n",
    "print(tokenizer.decode(targets[0]))\n",
    "\n",
    "\n",
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(targets)\n",
    "print(inputs.size())\n",
    "print(targets.size())\n",
    "\n",
    "max_input_seq = inputs.size(1)\n",
    "max_output_seq = targets.size(1)\n",
    "# max_input_seq = 1024\n",
    "# max_output_seq = 128\n",
    "print(f'Max input seq: {max_input_seq}')\n",
    "print(f\"Max output seq: {max_output_seq}\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f'Vocab size: {vocab_size}')"
   ],
   "id": "f4d8748c88ef3880",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self, inputs, targets):\n",
    "    self.inputs = inputs\n",
    "    self.targets = targets\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "dataset = CustomDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# for x, y in dataloader:\n",
    "#   print(f\"Inputs: {x.size()}, Targets: {y.size()}\")\n",
    "\n",
    "num_batches = len(dataloader)\n",
    "print(f\"Num batches: {num_batches}\")"
   ],
   "id": "95edd936a3001f70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emb_dim = 16\n",
    "n_layers = 6\n",
    "n_heads = 4\n",
    "dropout = 0.2"
   ],
   "id": "e7078e88b925e904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate mask for <pad> tokens\n",
    "def generate_padding_mask(idx):\n",
    "  \"\"\"unsqueese is used to adjust size of mask to self-attention mechanism\"\"\"\n",
    "  padding_mask = (idx == tokenizer.pad_token_id).unsqueeze(1).unsqueeze(2)  # unsqueese adds a new dimension 1 to defined position (from 0 to x)\n",
    "  return padding_mask\n",
    "\n",
    "# Generate a causal mask for target sequences\n",
    "def generate_tgt_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool().unsqueeze(0).unsqueeze(1)\n",
    "    return mask"
   ],
   "id": "80a7305ad0b75824",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CasualSelfAttention(nn.Module):\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "      assert emb_dim % n_heads == 0\n",
    "      self.num_heads = n_heads\n",
    "      self.head_dim = emb_dim // n_heads\n",
    "      self.query = nn.Linear(emb_dim, emb_dim)\n",
    "      self.key = nn.Linear(emb_dim, emb_dim)\n",
    "      self.value = nn.Linear(emb_dim, emb_dim)\n",
    "      self.fc = nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "  def forward(self, query, key, value, mask=None):\n",
    "      batch_size = query.size(0)\n",
    "      # print(mask)\n",
    "\n",
    "      def split_heads(x):\n",
    "          return x.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "      query, key, value = map(split_heads, [self.query(query), self.key(key), self.value(value)])\n",
    "\n",
    "      scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "      if mask is not None:\n",
    "          scores = scores.masked_fill(mask == 1, float('-inf'))  # if in mask appears True (e.g becasue of pad token) then -inf is added\n",
    "      attention_weights = torch.softmax(scores, dim=-1)\n",
    "      attention_output = torch.matmul(attention_weights, value).transpose(1, 2).contiguous()\n",
    "      attention_output = attention_output.view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "\n",
    "      return self.fc(attention_output)"
   ],
   "id": "70980c3433833cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ln_1 = nn.LayerNorm(emb_dim)  # layer norm before self attention\n",
    "    self.self_attention = CasualSelfAttention()  #multi-head attention\n",
    "    self.ln_2 = nn.LayerNorm(emb_dim)  # layer norm after self attention\n",
    "    self.feed_forward = nn.Sequential(\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(approximate='tanh'),  # TODO: why tanh ?\n",
    "        nn.Linear(emb_dim * 4, emb_dim))\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, src_mask):  # src_mask is mask for inputs\n",
    "    # 1.\n",
    "    x = self.ln_1(x)\n",
    "    attn_output = self.self_attention(x, x, x, src_mask)  # Self-attention without mask\n",
    "    x = self.dropout(x)\n",
    "    x = x + attn_output\n",
    "    # 2.\n",
    "    x = self.ln_2(x)\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = x + self.dropout(ff_output)\n",
    "    return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  # def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.input_emb = nn.Embedding(vocab_size, emb_dim)  # embeddings for max 50265 tokens with emb_dim dimension\n",
    "    self.pos_emb = nn.Embedding(max_input_seq, emb_dim)  # positional encoding (max_input_seq is the biggest article in input)\n",
    "    self.encoder_layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "  def forward(self, idx, src_mask):\n",
    "    B, T = idx.shape  # Batch size and sequence length\n",
    "\n",
    "    # Token embeddings\n",
    "    input_embeddings = self.input_emb(idx)  # Shape: [B, T, emb_dim]\n",
    "\n",
    "    # Positional embeddings\n",
    "    positions = torch.arange(0, T, device=idx.device)\n",
    "    positions = positions.unsqueeze(0)  # Shape: [1, T]\n",
    "\n",
    "    positional_embeddings = self.pos_emb(positions)  # Shape: [1, T, emb_dim]\n",
    "\n",
    "    # Combine token and positional embeddings\n",
    "    x = input_embeddings + positional_embeddings  # Shape: [B, T, emb_dim]\n",
    "\n",
    "    # Encoder layers\n",
    "    for layer in self.encoder_layers:\n",
    "      x = layer(x, src_mask)\n",
    "    return x\n",
    "\n",
    "# enc = Encoder()\n",
    "# for input, target in dataloader:\n",
    "#   print(input.shape)\n",
    "#   src_mask = generate_padding_mask(input)\n",
    "#   print(src_mask.shape)\n",
    "#   print(src_mask)\n",
    "#   x = enc(input, src_mask)\n",
    "#   print('Shape of x:', x.shape)\n",
    "#   break"
   ],
   "id": "50cff13d5ec6b80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "class DecoderLayer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ln_1 = nn.LayerNorm(emb_dim)\n",
    "    self.self_attention = CasualSelfAttention()\n",
    "    self.ln_2 = nn.LayerNorm(emb_dim)\n",
    "    self.cross_attention = CasualSelfAttention()\n",
    "    self.ln_3 = nn.LayerNorm(emb_dim)\n",
    "    self.feed_forward = nn.Sequential(\n",
    "        nn.Linear(emb_dim, emb_dim * 4),\n",
    "        nn.GELU(approximate='tanh'),\n",
    "        nn.Linear(emb_dim * 4, emb_dim)\n",
    "    )\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x, encoder_output, tgt_mask, src_mask):\n",
    "    # Self att\n",
    "    x = self.ln_1(x)\n",
    "    attn_output = self.self_attention(x, x, x, tgt_mask)\n",
    "    x = x + self.dropout(attn_output)\n",
    "\n",
    "    # Cross att\n",
    "    x = self.ln_2(x)\n",
    "    attn_output = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "    x = x + self.dropout(attn_output)\n",
    "\n",
    "    # Feed forward\n",
    "    x = self.ln_3(x)\n",
    "    ff_output = self.feed_forward(x)\n",
    "    x = x + self.dropout(ff_output)\n",
    "    return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.output_emb = nn.Embedding(vocab_size, emb_dim)  # embeddings for max 50265 tokens with emb_dim dimension\n",
    "    self.pos_emb = nn.Embedding(max_output_seq, emb_dim)  # positional encoding (max_input_seq is the biggest article in input)\n",
    "    self.decoder_layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "  def forward(self, x, encoder_output, tgt_mask, src_mask):\n",
    "    B, T = x.shape\n",
    "    x = self.output_emb(x)\n",
    "    # Positional embeddings\n",
    "    positions = torch.arange(0, T, device=x.device)\n",
    "    positions = positions.unsqueeze(0)  # Shape: [1, T]\n",
    "    positional_embeddings = self.pos_emb(positions)  # Shape: [1, T, emb_dim]\n",
    "    x = x + positional_embeddings\n",
    "\n",
    "    for layer in self.decoder_layers:\n",
    "      x = layer(x, encoder_output, tgt_mask, src_mask)\n",
    "    return x\n",
    "\n",
    "# TODO: lets understand decoder (and modify it)\n",
    "enc = Encoder()\n",
    "dec = Decoder()\n",
    "for input, target in dataloader:\n",
    "  src_mask = generate_padding_mask(input)\n",
    "  encoder_output = enc(input, src_mask)\n",
    "  print('Encoder output:', encoder_output.shape)\n",
    "\n",
    "  B, T = target.shape\n",
    "  tgt_ahead_mask = generate_tgt_mask(size=T)\n",
    "  print(tgt_ahead_mask.shape)\n",
    "  tgt_padding_mask = generate_padding_mask(target)\n",
    "  print(tgt_padding_mask.shape)\n",
    "  tgt_mask = tgt_ahead_mask + tgt_padding_mask\n",
    "  print('Final tgt mask:', tgt_mask)\n",
    "  # tgt_mask = tgt_ahead_mask & tgt_padding_mask\n",
    "\n",
    "  print(tgt_mask.shape)\n",
    "  decoder_output = dec(target, encoder_output, tgt_mask, src_mask)\n",
    "\n",
    "  print(decoder_output.shape)\n",
    "  break"
   ],
   "id": "de1742d4039bfb5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "class TransformerEncoderDecoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder()\n",
    "    self.decoder = Decoder()\n",
    "    self.ln_final =  nn.LayerNorm(emb_dim)\n",
    "    self.lm_head = nn.Linear(emb_dim, vocab_size, bias=False)\n",
    "\n",
    "  def forward(self, input, src_mask, tgt_mask, targets=None):\n",
    "    encoder_output = self.encoder(input, src_mask)\n",
    "    decoder_output = self.decoder(target, encoder_output, tgt_mask, src_mask)\n",
    "    x = self.ln_final(decoder_output)\n",
    "    logits = self.lm_head(x)\n",
    "    loss = None\n",
    "    if targets is not None:\n",
    "      # Compute loss\n",
    "      loss = F.cross_entropy(\n",
    "          logits.view(-1, logits.size(-1)),\n",
    "          targets.view(-1),\n",
    "          ignore_index=tokenizer.pad_token_id\n",
    "      )\n",
    "    return logits, loss\n",
    "\n",
    "  def generate_summary(self, input, src_mask=None, tgt_mask=None, max_length=50):\n",
    "      \"\"\"Generate a summary given input text.\"\"\"\n",
    "\n",
    "      # Encode the input text\n",
    "      encoder_output = self.encoder(input, src_mask)  # Shape: [1, seq_len, emb_dim]\n",
    "\n",
    "      # Start decoding with the <bos> token\n",
    "      generated = torch.ones(1, 1, device=device).long() * tokenizer.bos_token_id\n",
    "      print(generated)\n",
    "      print(encoder_output.shape)\n",
    "\n",
    "      for _ in range(max_length):\n",
    "          # Decode the current sequence\n",
    "          decoder_output = self.decoder(generated, encoder_output, src_mask=src_mask, tgt_mask=tgt_mask)  # Shape: [1, cur_len, emb_dim]\n",
    "\n",
    "          logits = self.lm_head(self.ln_final(decoder_output))  # Shape: [1, cur_len, vocab_size]\n",
    "          logits = logits[:, -1, :]  # Take logits of the last token\n",
    "\n",
    "          # Apply softmax to get probabilities and sample a token\n",
    "          probs = F.softmax(logits, dim=-1)\n",
    "          # plt.figure(figsize=(10, 6))\n",
    "          # plt.plot(probs.detach().numpy()[0])\n",
    "          # plt.xlabel('Token Index')\n",
    "          # plt.ylabel('Probability')\n",
    "          # plt.show()\n",
    "          # break\n",
    "\n",
    "          next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "\n",
    "          # Append the token to the generated sequence\n",
    "          generated = torch.cat([generated, next_token], dim=1)  # Shape: [1, cur_len + 1]\n",
    "\n",
    "          # Stop if <eos> token is generated\n",
    "          if next_token.item() == tokenizer.eos_token_id:\n",
    "              break\n",
    "\n",
    "      return generated"
   ],
   "id": "da8213c7d86f3c25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "# Hyperparameters\n",
    "\n",
    "learning_rate = 3e-4\n",
    "# gradient_accumulation_steps = 1  # Optional, to simulate larger batches\n",
    "# max_grad_norm = 1.0\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(dataloader)\n",
    "print('Total num training steps:', num_training_steps)\n",
    "print('Total num_epochs:', num_epochs)\n",
    "print('Dataset times revision:', num_epochs // len(dataloader))\n",
    "\n",
    "optimizer = AdamW(transformer.parameters(), lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)\n",
    "# Initialize the scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  transformer.train()  # Set model to training mode\n",
    "  for step, (input, target) in enumerate(dataloader):\n",
    "    optimizer.zero_grad()  # always start with zero grad!!\n",
    "\n",
    "    input = input.to(device)\n",
    "    target = target.to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    src_mask = None\n",
    "    tgt_mask = generate_tgt_mask(max_output_seq)\n",
    "    logits, loss = transformer(input=input, src_mask=src_mask, tgt_mask=tgt_mask, targets=target)\n",
    "    # print(logits)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)  # also from GPT-3 paper\n",
    "    optimizer.step()  # update parameters to decrease loss\n",
    "\n",
    "    # Scheduler step\n",
    "    lr_scheduler.step()\n",
    "\n",
    "\n",
    "  # Log progress\n",
    "  if epoch % 2 == 0:\n",
    "      print(f\"Epoch [{epoch}], Loss: {loss.item():.4f}\")"
   ],
   "id": "4028808ab2fb83db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "# Generate\n",
    "transformer.eval()\n",
    "\n",
    "article = news[0]\n",
    "inp = tokenizer(article, max_length=1024, return_tensors=\"pt\")\n",
    "summary_ids = transformer.generate_summary(input=inp['input_ids'], max_length=20)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)"
   ],
   "id": "4c4f842ac3e7af1a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
