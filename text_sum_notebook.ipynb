{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-09T09:57:07.761945Z",
     "start_time": "2025-02-09T09:57:05.781665Z"
    }
   },
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check if GPU is available\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "# device = 'cpu'\n",
    "# import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "GPU count: 1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T09:57:16.212889Z",
     "start_time": "2025-02-09T09:57:07.761945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Load the entire dataset\n",
    "# dataset = load_dataset(\"...\", verification_mode=VerificationMode.NO_CHECKS)\n",
    "ds = load_dataset(\"argilla/cnn-dailymail-summaries\", split='train')\n",
    "print(ds)"
   ],
   "id": "63a134c06afa03ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['article', 'highlights', 'id', 'summary', 'distilabel_metadata', 'model_name'],\n",
      "    num_rows: 287113\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:11:45.003637Z",
     "start_time": "2025-02-09T09:57:16.384943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Tokens <s> and </s> are added from default in BartTokenizer\"\"\"\n",
    "\n",
    "# Padding was requried due to different size of inputs (each article is different): https://huggingface.co/docs/transformers/en/\n",
    "# val = 2 ** 18\n",
    "val = 2 ** 17\n",
    "print(val)\n",
    "inputs = tokenizer(ds['article'][:val], padding='longest', truncation=True, max_length=1024)['input_ids']\n",
    "targets = tokenizer(ds['highlights'][:val], padding='longest', truncation=True, max_length=128)[\n",
    "    'input_ids']  # --> padding to the longest text but shorter or equal max_len (128)\n",
    "print(tokenizer.decode(inputs[0]))\n",
    "print(tokenizer.decode(targets[0]))\n",
    "\n",
    "# inputs = torch.tensor(inputs)\n",
    "# targets = torch.tensor(targets)\n",
    "# Efficient conversion to CUDA tensors\n",
    "inputs = torch.as_tensor(inputs, device=device)\n",
    "targets = torch.as_tensor(targets, device=device)\n",
    "print(inputs.size())\n",
    "print(targets.size())\n",
    "\n",
    "max_input_seq = inputs.size(1)\n",
    "max_output_seq = targets.size(1)\n",
    "# max_input_seq = 1024\n",
    "# max_output_seq = 128\n",
    "print(f'Max input seq: {max_input_seq}')\n",
    "print(f\"Max output seq: {max_output_seq}\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f'Vocab size: {vocab_size}')"
   ],
   "id": "f4d8748c88ef3880",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131072\n",
      "<s>Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "<s>Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\n",
      "Young actor says he has no plans to fritter his cash away .\n",
      "Radcliffe's earnings from first five Potter films have been held in trust fund .</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "torch.Size([131072, 1024])\n",
      "torch.Size([131072, 128])\n",
      "Max input seq: 1024\n",
      "Max output seq: 128\n",
      "Vocab size: 50265\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:34:25.085441Z",
     "start_time": "2025-02-09T10:34:25.078510Z"
    }
   },
   "cell_type": "code",
   "source": "batch_size = 16",
   "id": "42ef036c9e4a4bc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:34:25.701490Z",
     "start_time": "2025-02-09T10:34:25.687957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)\n",
    "dataset = CustomDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# for x, y in dataloader:\n",
    "#   print(f\"Inputs: {x.size()}, Targets: {y.size()}\")\n",
    "\n",
    "num_batches = len(dataloader)\n",
    "print(f\"Num batches: {num_batches}\")"
   ],
   "id": "95edd936a3001f70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num batches: 8192\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](images/transformer_architecture.png)\n",
   "id": "a7ce4c59da5b56f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:34:27.059338Z",
     "start_time": "2025-02-09T10:34:27.047443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_padding_mask(idx: torch.Tensor, tgt_size, src_size):\n",
    "    mask = (idx != tokenizer.pad_token_id).bool()\n",
    "    mask = mask.unsqueeze(1).expand(-1, tgt_size, src_size).unsqueeze(1).to(device)  # (batch_size, seq_len, seq_len)\n",
    "    return mask\n",
    "\n",
    "def get_tgt_mask(idy: torch.Tensor):\n",
    "    mask = torch.triu(torch.ones(idy.size(1), idy.size(1)).bool(), diagonal=1).logical_not()\n",
    "    mask = mask.expand(idy.size(0), -1, -1).unsqueeze(1).to(device)\n",
    "    return mask"
   ],
   "id": "a0551956b6dc2709",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:34:27.664343Z",
     "start_time": "2025-02-09T10:34:27.656725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model, 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        q, k, v: (N, H, L, E - batch_size, n_heads, seq_len, head_dim)\n",
    "        mask: (N, L, S - batch_size, tgt_seq_len, src_seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, max_sequence_length, d_model = x.size()  # B,T,C\n",
    "\n",
    "        # print('Mask:', mask.shape)\n",
    "        qkv = self.qkv_layer(x)  # B,T,C=C*3\n",
    "        qkv = qkv.reshape(batch_size, max_sequence_length, self.num_heads, 3 * self.head_dim)  # B,T,N_HEADS,C=DIM//N_HEADS*3\n",
    "        qkv = qkv.permute(0, 2, 1, 3)  # permute swaps the places\n",
    "        q, k, v = qkv.chunk(3, dim=-1)  # B,N_HEADS,T,C=C/3\n",
    "        values = F.scaled_dot_product_attention(q, k, v, mask)  # B,T,N_HEADS,C\n",
    "        values = values.reshape(batch_size, max_sequence_length, self.num_heads * self.head_dim)  # B,T,C\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ],
   "id": "857c174cbe1c950f",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:34:28.651416Z",
     "start_time": "2025-02-09T10:34:28.641007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ffn_hidden),\n",
    "            nn.GELU(approximate='tanh'),\n",
    "            nn.Linear(ffn_hidden, d_model))\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        residual_x = x\n",
    "        # print(\"------- ATTENTION 1 ------\")\n",
    "        x = self.attention(x, src_mask)\n",
    "        # print(\"------- DROPOUT 1 ------\")\n",
    "        x = self.dropout1(x)\n",
    "        # print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x\n",
    "        # print(\"------- ATTENTION 2 ------\")\n",
    "        x = self.ffn(x)\n",
    "        # print(\"------- DROPOUT 2 ------\")\n",
    "        x = self.dropout2(x)\n",
    "        # print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, dropout, num_layers, vocab_size, max_input_seq):\n",
    "        super().__init__()\n",
    "        self.input_emb = nn.Embedding(vocab_size, d_model,\n",
    "                                      device=device)  # embeddings for max 50265 tokens with emb_dim dimension\n",
    "        self.pos_emb = nn.Embedding(max_input_seq, d_model,\n",
    "                                    device=device)  # positional encoding (max_input_seq is the biggest article in input)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, ffn_hidden, num_heads, dropout).to(device) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        B, T = x.shape  # Batch size and sequence length\n",
    "\n",
    "        # Token embeddings\n",
    "        input_embeddings = self.input_emb(x)  # Shape: [B, T, emb_dim]\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = torch.arange(0, T, device=x.device)\n",
    "        positions = positions.unsqueeze(0)  # Shape: [1, T]\n",
    "\n",
    "        positional_embeddings = self.pos_emb(positions)  # Shape: [1, T, emb_dim]\n",
    "\n",
    "        # Combine token and positional embeddings\n",
    "        x = input_embeddings + positional_embeddings  # Shape: [B, T, emb_dim]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x"
   ],
   "id": "5259d982a2612386",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:34:29.149570Z",
     "start_time": "2025-02-09T10:34:29.141094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model, 2 * d_model)  # 1024\n",
    "        self.q_layer = nn.Linear(d_model, d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, y, mask=None):\n",
    "        \"\"\"\n",
    "        q: (N, H, L, E - batch_size, n_heads, tgt_seq_len, head_dim)\n",
    "        kv: (N, H, S, E - batch_size, n_heads, src_seq_len, head_dim)\n",
    "        mask: (N, L, S - batch_size, tgt_seq_len, src_seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, d_model = x.size()  # x (encoder output)\n",
    "        _, target_sequence_length, _ = y.size()  # y (decoder input)\n",
    "        kv = self.kv_layer(x)\n",
    "        q = self.q_layer(y)\n",
    "\n",
    "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
    "        q = q.reshape(batch_size, target_sequence_length, self.num_heads, self.head_dim)  # Use target_sequence_length\n",
    "        kv = kv.permute(0, 2, 1, 3)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "\n",
    "        values = F.scaled_dot_product_attention(q, k, v, mask)\n",
    "        values = values.reshape(batch_size, target_sequence_length, d_model)\n",
    "        out = self.linear_layer(values)\n",
    "        return out"
   ],
   "id": "3c29ef772df009c6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:34:29.757067Z",
     "start_time": "2025-02-09T10:34:29.734743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.cross_attention = CrossAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ffn_hidden),\n",
    "            nn.GELU(approximate='tanh'),\n",
    "            nn.Linear(ffn_hidden, d_model))\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_output, y, tgt_mask=None, cross_mask=None):\n",
    "        residual_y = y\n",
    "        # print(\"MASKED SELF ATTENTION\")\n",
    "        y = self.self_attention(y, mask=tgt_mask)\n",
    "        # print(\"DROP OUT 1\")\n",
    "        y = self.dropout1(y)\n",
    "        # print(\"ADD + LAYER NORMALIZATION 1\")\n",
    "        y = self.norm1(y + residual_y)\n",
    "        residual_y = y\n",
    "        # print(\"CROSS ATTENTION\")\n",
    "        y = self.cross_attention(encoder_output, y, cross_mask)\n",
    "        # print(\"DROP OUT 2\")\n",
    "        y = self.dropout2(y)\n",
    "        # print(\"ADD + LAYER NORMALIZATION 2\")\n",
    "        y = self.norm2(y + residual_y)\n",
    "        residual_y = y\n",
    "        # print(\"FEED FORWARD 1\")\n",
    "        y = self.ffn(y)\n",
    "        # print(\"DROP OUT 3\")\n",
    "        y = self.dropout3(y)\n",
    "        # print(\"ADD + LAYER NORMALIZATION 3\")\n",
    "        y = self.norm3(y + residual_y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, dropout, num_layers, vocab_size, max_output_seq):\n",
    "        super().__init__()\n",
    "        self.output_emb = nn.Embedding(vocab_size, d_model,\n",
    "                                      device=device)\n",
    "        self.pos_emb = nn.Embedding(max_output_seq, d_model,\n",
    "                                    device=device)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, ffn_hidden, num_heads, dropout).to(device) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, encoder_output, tgt_mask=None, cross_mask=None):\n",
    "        B, T = x.shape  # Batch size and sequence length\n",
    "\n",
    "        # Token embeddings\n",
    "        output_embeddings = self.output_emb(x)  # Shape: [B, T, emb_dim]\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = torch.arange(0, T, device=x.device)\n",
    "        positions = positions.unsqueeze(0)  # Shape: [1, T]\n",
    "\n",
    "        positional_embeddings = self.pos_emb(positions)  # Shape: [1, T, emb_dim]\n",
    "\n",
    "        # Combine token and positional embeddings\n",
    "        x = output_embeddings + positional_embeddings  # Shape: [B, T, emb_dim]\n",
    "        for layer in self.layers:\n",
    "            x = layer(encoder_output, x, tgt_mask, cross_mask)\n",
    "        return x"
   ],
   "id": "6bbb5d063bba8f79",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:34:30.134734Z",
     "start_time": "2025-02-09T10:34:30.128936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "dropout = 0.2\n",
    "ffn_hidden = d_model * 4\n",
    "num_layers = 8"
   ],
   "id": "22581b14aab3b50e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:34:32.444711Z",
     "start_time": "2025-02-09T10:34:32.437862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, dropout, num_layers, vocab_size, max_output_seq, max_input_seq):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, dropout, num_layers, vocab_size, max_input_seq)\n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, dropout, num_layers, vocab_size, max_output_seq)\n",
    "        self.ln_final =  nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, inputs, targets, src_mask=None, tgt_mask=None, cross_mask=None):\n",
    "        encoder_output = self.encoder(inputs, src_mask=src_mask)\n",
    "        out = self.decoder(targets, encoder_output, tgt_mask=tgt_mask, cross_mask=cross_mask)\n",
    "        out = self.ln_final(out)\n",
    "        logits = self.lm_head(out)\n",
    "        return logits\n",
    "\n"
   ],
   "id": "e9b4db69597059a0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:34:34.896509Z",
     "start_time": "2025-02-09T10:34:34.266073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformer = MyTransformer(d_model, ffn_hidden, num_heads, dropout, num_layers, vocab_size, max_output_seq, max_input_seq)\n",
    "transformer = transformer.to(device)"
   ],
   "id": "3ef1d76b739c45a7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T00:56:16.557634Z",
     "start_time": "2025-02-09T10:34:44.172356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm import tqdm  # For a progress bar\n",
    "# Hyperparameters\n",
    "\n",
    "learning_rate = 3e-4\n",
    "# gradient_accumulation_steps = 1  # Optional, to simulate larger batches\n",
    "# max_grad_norm = 1.0\n",
    "# num_epochs = 10\n",
    "# num_training_steps = num_epochs * len(dataloader)\n",
    "# print('Total num training steps:', num_training_steps)\n",
    "# print('Total num_epochs:', num_epochs)\n",
    "# print('Dataset times revision:', num_epochs // len(dataloader))\n",
    "\n",
    "optimizer = AdamW(transformer.parameters(), lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "\n",
    "num_epochs = 5\n",
    "gradient_accumulation_steps = 2  # Simulates larger batch sizes\n",
    "\n",
    "total_steps = num_epochs * len(dataloader)\n",
    "print(f\"Total Training Steps: {total_steps}\")\n",
    "# Initialize the scheduler\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    transformer.train()  # Set model to training mode\n",
    "    epoch_loss = 0  # Track loss per epoch\n",
    "\n",
    "    # Progress bar for tracking\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for step, (input, target) in progress_bar:\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        input, target = input.to(device), target.to(device)  # Move to GPU\n",
    "        #TODO: to add masks\n",
    "\n",
    "        input_padding_mask = get_padding_mask(input, input.size(1), input.size(1)) # Encoder padding mask\n",
    "        target_padding_mask = get_padding_mask(target, target.size(1), target.size(1))  # Decoder padding mask\n",
    "        target_mask = get_tgt_mask(target) & target_padding_mask # Decoder casual mask for target\n",
    "        cross_mask = get_padding_mask(input, target.size(1), input.size(1))  # Cross mask in Decoder\n",
    "        #\n",
    "        # print('Input padding mask:', input_padding_mask.shape)\n",
    "        # print('Target mask:', target_mask.shape)\n",
    "        # print('Cross mask:', cross_mask.shape)\n",
    "\n",
    "        logits = transformer(inputs=input, targets=target, src_mask=input_padding_mask, tgt_mask=target_mask, cross_mask=cross_mask)\n",
    "\n",
    "\n",
    "        loss = F.cross_entropy(\n",
    "          logits.view(-1, logits.size(-1)),  # Shape: (B * T, vocab_size)\n",
    "          target.view(-1),  # Shape: (B * T)\n",
    "          ignore_index=tokenizer.pad_token_id,\n",
    "      )\n",
    "        loss = loss / gradient_accumulation_steps  # Normalize loss for accumulation\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), 1.0)  # Prevent exploding gradients\n",
    "\n",
    "        # Update weights after accumulating steps\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        # Update loss tracking\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    # Log epoch-level loss\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training Completed ✅\")"
   ],
   "id": "a6524749cc59df90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Steps: 40960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 8192/8192 [2:53:09<00:00,  1.27s/it, Loss=0.2098]  \n",
      "Epoch 2: 100%|██████████| 8192/8192 [2:55:21<00:00,  1.28s/it, Loss=0.2401]  \n",
      "Epoch 3: 100%|██████████| 8192/8192 [2:52:35<00:00,  1.26s/it, Loss=0.3188]  \n",
      "Epoch 4: 100%|██████████| 8192/8192 [2:50:18<00:00,  1.25s/it, Loss=0.3113]  \n",
      "Epoch 5: 100%|██████████| 8192/8192 [2:50:06<00:00,  1.25s/it, Loss=0.2828]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Completed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T06:22:58.680463Z",
     "start_time": "2025-02-10T06:22:58.672252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_summary(model: MyTransformer, input, max_len=40):\n",
    "    model.eval()\n",
    "    # Ensure input is on the correct device\n",
    "    input = input.to(device)\n",
    "\n",
    "    # Get encoder output\n",
    "    with torch.no_grad():\n",
    "        input_padding_mask = get_padding_mask(input, input.size(1), input.size(1))  # Encoder padding mask\n",
    "        encoder_output = model.encoder(input, src_mask=input_padding_mask)\n",
    "        generated = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            # TODO: add masks\n",
    "            target_padding_mask = get_padding_mask(generated, generated.size(1), generated.size(1))  # Decoder padding mask\n",
    "            target_mask = get_tgt_mask(generated) & target_padding_mask # Decoder casual mask for target\n",
    "            cross_mask = get_padding_mask(input, generated.size(1), input.size(1))  # Cross mask in Decoder\n",
    "\n",
    "            # Get model predictions\n",
    "\n",
    "            logits = model.decoder(\n",
    "                generated, encoder_output,\n",
    "                tgt_mask=target_mask,\n",
    "                cross_mask=cross_mask\n",
    "            )\n",
    "            # Get the last predicted token (greedy decoding)\n",
    "            next_token_logits = logits[:, -1, :]  # Shape: [batch_size, vocab_size]\n",
    "            # probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "            # next_token = torch.multinomial(probs, num_samples=1)  # Select most probable token\n",
    "\n",
    "            # Append token to generated sequence\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "            # Stop generating if the <EOS> token is reached\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "     # Convert token IDs to text\n",
    "    summary_text = tokenizer.decode(generated.squeeze(0).tolist(), skip_special_tokens=True)\n",
    "    return summary_text\n"
   ],
   "id": "89537b578441fc22",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T06:24:02.383213Z",
     "start_time": "2025-02-10T06:24:01.797340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "inp = tokenizer.decode(inputs[0], skip_special_tokens=True)\n",
    "# inp = 'This'\n",
    "print(inp[:100])\n",
    "test_input = torch.tensor([tokenizer.encode(inp)], device=device)\n",
    "summary = generate_summary(transformer, test_input)\n",
    "print(\"Generated Summary:\", summary)\n"
   ],
   "id": "e3767518f83c27f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as\n",
      "Generated Summary:  some never month? his while \"C their government always 8 own e., play school across didI early will today National% while L That working 2018 any $ both As United making United making lead long\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
